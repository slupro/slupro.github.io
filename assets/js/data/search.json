[ { "title": "算法学习笔记", "url": "/posts/Algorithm-in-one-page/", "categories": "学习笔记", "tags": "Algorithm", "date": "2021-12-04 02:00:00 +1100", "snippet": "之前在极客时间学习算法时做的笔记，截图很多引用自王争的《数据结构与算法之美》，推荐大家可以买书或者去极客时间学习。时间复杂度(O)图链表写链表的注意事项重点留意边界条件处理经常用来检查链表是否正确的边界4个边界条件： 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个节点时，代码是否能正常工作？ 如果链表只包含两个节点时，代码是否能正常工作？ 代码逻辑在处理头尾节点时是否能正常工作？多练习，画图帮助思考5个常见的链表操作： 单链表反转 链表中环的检测 两个有序链表合并 删除链表倒数第n个节点 求链表的中间节点添加，打印 若用函数参数返回新产生的链表，则需要使用指向指针的指针！下面的例子中的函数使用了可变长参数#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdarg.h&amp;gt;struct ListNode { int value; ListNode *next;};void PrintList(ListNode *p){ ListNode *t=p; while(t) { printf(&quot;%d-&amp;gt;&quot;, t-&amp;gt;value); t = t-&amp;gt;next; } printf(&quot;NULL\\n&quot;);}ListNode* AddToList(ListNode *p, int nArgs, ...){ va_list args; int value; ListNode* head = p; va_start(args, nArgs); if (nArgs &amp;lt;= 0) return head; for (int i=0; i&amp;lt;nArgs; i++) { value=va_arg(args, int); ListNode *pNewNode = new ListNode; pNewNode-&amp;gt;value = value; pNewNode-&amp;gt;next = NULL; if (p == NULL) { p = pNewNode; head = pNewNode; } else { while(p-&amp;gt;next) p = p-&amp;gt;next; p-&amp;gt;next = pNewNode; } } va_end(args); return head;}void AddValueToList(ListNode **p, int value){ ListNode *newp = new ListNode(); newp-&amp;gt;value = value; newp-&amp;gt;next = NULL; if (*p == NULL) { *p = newp; } else { ListNode *pt = *p; while(pt-&amp;gt;next != NULL) pt = pt-&amp;gt;next; pt-&amp;gt;next = newp; } return;}int main(){ ListNode *pl = AddToList(NULL, 5, 1, 2, 3, 4, 5); PrintList(pl); ListNode *p; AddValueToList(&amp;amp;p, 2); AddValueToList(&amp;amp;p, 3); AddValueToList(&amp;amp;p, 4); PrintList(p); return 0;}反转链表 三个变量，分别指向 前，中，后需要考虑到程序到可靠性：输入NULL，单个节点，多个节点的ListNodeListNode *reverseList(ListNode *head){ if (head == NULL) return NULL; ListNode *pre = head, *curr = head, *post = head-&amp;gt;next; pre-&amp;gt;next = NULL; while (post != NULL) { curr = post; post = curr-&amp;gt;next; curr-&amp;gt;next = pre; pre = curr; } return curr;} 递归方式：先走到链表底部，再一个个反转ListNode* ReverseList2(ListNode *p){ if (p == NULL || p-&amp;gt;next == NULL) return p; ListNode *pt = ReverseList2(p-&amp;gt;next); p-&amp;gt;next-&amp;gt;next = p; p-&amp;gt;next = NULL; return pt;} 网上的实现：使用递归和非递归两种方式#include&amp;lt;iostream&amp;gt;using namespace std;struct node{ int val; struct node* next; node(int x) :val(x){}};/***非递归方式***/node* reverseList(node* H){ if (H == NULL || H-&amp;gt;next == NULL) //链表为空或者仅1个数直接返回 return H; node* p = H, *newH = NULL; while (p != NULL) //一直迭代到链尾 { node* tmp = p-&amp;gt;next; //暂存p下一个地址，防止变化指针指向后找不到后续的数 p-&amp;gt;next = newH; //p-&amp;gt;next指向前一个空间 newH = p; //新链表的头移动到p，扩长一步链表 p = tmp; //p指向原始链表p指向的下一个空间 } return newH;}/***递归方式***/node* In_reverseList(node* H){ if (H == NULL || H-&amp;gt;next == NULL) //链表为空直接返回，而H-&amp;gt;next为空是递归基 return H; node* newHead = In_reverseList(H-&amp;gt;next); //一直循环到链尾 H-&amp;gt;next-&amp;gt;next = H; //翻转链表的指向 H-&amp;gt;next = NULL; //记得赋值NULL，防止链表错乱 return newHead; //新链表头永远指向的是原链表的链尾}int main(){ node* first = new node(1); node* second = new node(2); node* third = new node(3); node* forth = new node(4); node* fifth = new node(5); first-&amp;gt;next = second; second-&amp;gt;next = third; third-&amp;gt;next = forth; forth-&amp;gt;next = fifth; fifth-&amp;gt;next = NULL; //非递归实现 node* H1 = first; H1 = reverseList(H1); //翻转 //递归实现 node* H2 = H1; //请在此设置断点查看H1变化，否则H2再翻转，H1已经发生变化 H2 = In_reverseList(H2); //再翻转 return 0;}从头到尾打印链表 FILO先入后出，则可以用栈，也可以用递归，递归本质上就是一个栈结构。void PrintReverseList(ListNode *p){ if (p-&amp;gt;next != NULL) PrintReverseList(p-&amp;gt;next); printf(&quot;%d &quot;, p-&amp;gt;value);}删除链表节点简单做法，遍历节点，那么时间复杂度就是O(n)。优化做法，O(1)，有可能O(1)。void DeleteNode(ListNode **pp, ListNode *pToBeDeleted);需要考虑： 删除最后一个节点 删除唯一的节点 节点不在list里面 特殊输入测试，比如NULLvoid DeleteNode(ListNode **pp, ListNode *pToBeDeleted, bool bCheckNode = false){ if (*pp == pToBeDeleted) { if ((*pp)-&amp;gt;next == NULL) { delete pToBeDeleted; *pp = NULL; } else { *pp = (*pp)-&amp;gt;next; delete pToBeDeleted; } } else if (bCheckNode || pToBeDeleted-&amp;gt;next == NULL) { ListNode *p = *pp; while(p-&amp;gt;next != NULL) { if (p-&amp;gt;next == pToBeDeleted) { p-&amp;gt;next = pToBeDeleted-&amp;gt;next; delete pToBeDeleted; break; } p = p-&amp;gt;next; } } else { pToBeDeleted-&amp;gt;value = pToBeDeleted-&amp;gt;next-&amp;gt;value; pToBeDeleted-&amp;gt;next = pToBeDeleted-&amp;gt;next-&amp;gt;next; }}从后往前找第N个节点（双指针） 使用两个指针// n=1, means the last one.ListNode* GetNodeFromEnd(ListNode *p, unsigned int n){ if (p == NULL || n == 0) return NULL; ListNode *p1 = p; ListNode *p2 = p; for (unsigned int i=1; i&amp;lt;n; i++) { p2 = p2-&amp;gt;next; if (p2 == NULL) return p2; } while(p2-&amp;gt;next != NULL) { p1 = p1-&amp;gt;next; p2 = p2-&amp;gt;next; } return p1;} 扩展问题：1. 找链表的中间节点，2. 判断是否形成了闭环都可以使用双指针，一个每次走一步，一个每次走两步。合并两个有序链表 用两个指针，一个指向头，一个在两个链表一步步比较下去// p1, p2 is asc order.ListNode *MergeList(ListNode *p1, ListNode *p2){ ListNode *ph = NULL; // 指向头 ListNode *pf = NULL; // 串联使用的变量 while(p1 != NULL || p2 != NULL) { if (p1 != NULL &amp;amp;&amp;amp; p2 != NULL) { if (p1-&amp;gt;value &amp;gt; p2-&amp;gt;value) { if (ph == NULL) { ph = p2; } else { pf-&amp;gt;next = p2; } pf = p2; p2 = p2-&amp;gt;next; } else { if (ph == NULL) { ph = p1; } else { pf-&amp;gt;next = p1; } pf = p1; p1 = p1-&amp;gt;next; } } else if (p1 == NULL) { if (ph == NULL) return p2; else { pf-&amp;gt;next = p2; return ph; } } else if (p2 == NULL) { if (ph == NULL) return p1; else { pf-&amp;gt;next = p1; return ph; } } } return ph;} 使用递归ListNode *MergeList2(ListNode *p1, ListNode *p2){ if (p1 == NULL) return p2; if (p2 == NULL) return p1; ListNode *pr = NULL; if (p1-&amp;gt;value &amp;lt; p2-&amp;gt;value) { pr = p1; pr-&amp;gt;next = MergeList2(p1-&amp;gt;next, p2); } else { pr = p2; pr-&amp;gt;next = MergeList2(p1, p2-&amp;gt;next); } return pr;}找两个链表中的第一个公共节点 单向链表，那么有公共节点的话，后面每一个节点都相同。两个链表只能是Y型。找到长度，一个先走。排序评价一个排序算法，需要考虑：执行效率，内存消耗，稳定性。 稳定性是说排序后相同的元素，在排序后的顺序不发生变化。 原地排序是说不需要分配多余的内存，某些情况可能很重要。O(n^2)冒泡 bubble sort 从前往后，每次比较两数，把最小或者最大的数交换到后面去。改进：有可能提前排好。如果没有交换发生，就说明已排好了。时间复杂度是O(n^2)// 冒泡排序，a表示数组，n表示数组大小public void bubbleSort(int[] a, int n) { if (n &amp;lt;= 1) return; for (int i = 0; i &amp;lt; n; ++i) { // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &amp;lt; n - i - 1; ++j) { if (a[j] &amp;gt; a[j+1]) { // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 } } if (!flag) break; // 没有数据交换，提前退出 }}插入排序 Insertion sort 动态地往有序集合中添加数据，通过这种方式保持集合中的数据一直有序。时间复杂度是O(n^2)，但速度比冒泡快，因为冒泡排序要中数据交换的次数更多。// 插入排序，a表示数组，n表示数组大小public void insertionSort(int[] a, int n) { if (n &amp;lt;= 1) return; for (int i = 1; i &amp;lt; n; ++i) { int value = a[i]; int j = i - 1; // 查找插入的位置 for (; j &amp;gt;= 0; --j) { if (a[j] &amp;gt; value) { a[j+1] = a[j]; // 数据移动 } else { break; } } a[j+1] = value; // 插入数据(注意此时是j+1) }}选择排序 Selection Sort 类似插入排序，但是选择排序每次会从未排序区间找到最小的元素，将其放到已排序区间的末尾。O(nlogn)归并排序 Merge Sort 先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排序好的两部分合并在一起，这样整个数组就都有序了。归并使用的是分治思想，将一个大问题分解成小问题来解决，小问题解决了，大问题也就解决了。递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))终止条件：p &amp;gt;= r 不用再继续分解快速排序 Quick Sort 和递归排序一样，也是分治的思想。在要排序的数组下标从p到r之间，选任意一个数据作为pivot分区点。遍历p到r，将小于pivot的放左边，大于的放右边。递推公式：quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)终止条件：p &amp;gt;= r 上面的思路需要额外的内存空间，所以不是原地排序算法了。下面的伪代码实现了原地排序// 使用游标 i 将A[p...r-1]分成两部分，A[p...i-1]的元素都是小于pivot的，当作已处理区，A[i...r-1]是未处理区。每次都从A[i...r-1]中取出一个元素A[j]，与pivot比较，小于pivot的，将其加入已处理区的尾部，也就是A[i]的位置。partition(A, p, r) { pivot := A[r] i := p for j := p to r-1 do { if A[j] &amp;lt; pivot { swap A[i] with A[j] i := i+1 } } swap A[i] with A[r] return i} 如何优化快速排序：最理想的分区点是被分开的两个分区中，数据的数量差不多。 三数取中，或者多个数取中，做为分区点。 随机取一个元素做分区点。堆堆的基本信息(完全二叉树，插入删除都是O(logn)) 堆是一种特殊的树。满足下面两点，就是堆：第一点，堆必须是一个完全二叉树。完全二叉树要求，除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列。第二点，堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值。 注意，并不要求上层的节点比下层任一节点大，只要求比自己的子节点大。 对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。完全二叉树适合用数组来存储。数组中下标为 i 的节点的左子节点，就是下标为 i∗2 的节点，右子节点就是下标为 i∗2+1 的节点，父节点就是下标为 i/2 的节点。往堆中插入数据可以先把新插入的数据放最后，让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。从堆中删除数据我们把最后一个节点放到要删除的数据位置，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。因为我们移除的是数组中的最后一个元素，而在堆化的过程中，都是交换操作，不会出现数组中的“空洞”，所以这种方法堆化之后的结果，肯定满足完全二叉树的特性。 在堆中插入和删除数据的时间复杂度都是O(logn)。堆排序O(原地排序，nlogn)我们将下标从 n/2 到 1 的节点，依次进行从上到下的堆化操作，然后就可以将数组中的数据组织成堆这种数据结构。接下来，我们迭代地将堆顶的元素放到堆的末尾，并将堆的大小减一，然后再堆化，重复这个过程，直到堆中只剩下一个元素，整个数组中的数据就都有序排列了。整个堆排序的过程，都只需要极个别临时存储空间，所以堆排序是原地排序算法。堆排序包括建堆和排序两个操作，建堆过程的时间复杂度是 O(n)，排序过程的时间复杂度是 O(nlogn)，所以，堆排序整体的时间复杂度是 O(nlogn)。在实际开发中，为什么快速排序要比堆排序性能好？ 堆排序数据访问的方式没有快速排序友好。跳着访问，对 CPU 缓存是不友好的。 对于同样的数据，在排序过程中，堆排序算法的数据交换次数要多于快速排序。堆的应用 优先级队列 a. 合并多个有序文件（各个文件都取出第一个，然后比较，再在相关的文件取下一个，比较的时候就可以用堆，插入、删除都是O(logn)） b. 高性能定时器（多个定时任务需要执行，构造一个小顶堆，堆顶是最近需要执行的。） 求 Top K 可以维护一个大小为 K 的 小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前 K 大数据了。 求 中位数或某一百分位的数 如果我们面对的是动态数据集合，中位数在不停地变动，如果再用先排序的方法，每次询问中位数的时候，都要先进行排序，那效率就不高了。 如要求排在80%位的数据。那么就维护两个堆，假设当前总数据的个数是 n，大顶堆中保存 n乘80% 个数据，小顶堆中保存 n乘20% 个数据。大顶堆堆顶的数据就是我们要找的 80%的数据。每次插入一个数据的时候，我们要判断这个数据跟大顶堆和小顶堆堆顶数据的大小关系，然后决定插入到哪个堆中。如果这个新插入的数据比大顶堆的堆顶数据小，那就插入大顶堆；如果这个新插入的数据比小顶堆的堆顶数据大，那就插入小顶堆。同时需维护两个堆中20:80的比例，可能需要将一个堆中的数据移动到另外一个中。 通过这样的方法，每次插入数据，可能会涉及几个数据的堆化操作，所以时间复杂度是 O(logn)。但取值时直接返回大顶堆中的堆顶数据即可，时间复杂度是 O(1)。堆的相关代码public class Heap { private int[] a; // 数组，从下标1开始存储数据 private int n; // 堆可以存储的最大数据个数 private int count; // 堆中已经存储的数据个数 public Heap(int capacity) { a = new int[capacity + 1]; n = capacity; count = 0; } public void insert(int data) { if (count &amp;gt;= n) return; // 堆满了 ++count; a[count] = data; int i = count; while (i/2 &amp;gt; 0 &amp;amp;&amp;amp; a[i] &amp;gt; a[i/2]) { // 自下往上堆化 swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素 i = i/2; } } private static void buildHeap(int[] a, int n) { for (int i = n/2; i &amp;gt;= 1; --i) { heapify(a, n, i); } } public void removeMax() { if (count == 0) return -1; // 堆中没有数据 a[1] = a[count]; --count; heapify(a, count, 1); } private void heapify(int[] a, int n, int i) { // 自上往下堆化 while (true) { int maxPos = i; if (i*2 &amp;lt;= n &amp;amp;&amp;amp; a[i] &amp;lt; a[i*2]) maxPos = i*2; if (i*2+1 &amp;lt;= n &amp;amp;&amp;amp; a[maxPos] &amp;lt; a[i*2+1]) maxPos = i*2+1; if (maxPos == i) break; swap(a, i, maxPos); i = maxPos; } } // n表示数据的个数，数组a中的数据从下标1到n的位置。 public static void sort(int[] a, int n) { buildHeap(a, n); int k = n; while (k &amp;gt; 1) { swap(a, 1, k); --k; heapify(a, k, 1); } }}buildHeap() 进行堆化的时候，我们对下标从 n/2 开始到 1 的数据进行堆化，下标是 n/2+1 到 n 的节点是叶子节点，我们不需要堆化。实际上，对于完全二叉树来说，下标从 n/2+1 到 n 的节点都是叶子节点。O(n)桶排序 Bucket Sort 如果要排序的数据有n个，把它们划分到m个有序的桶内，每个桶内的数据再单独进行排序。排完序后，按顺序把每个桶里的数据依次取出，组成的序列就是有序的了。当m接近n时，时间复杂度接近O(n)。极端情况下，数据被划分到一个桶中，就退化为O(nlogn)。如果某个区间划分的数据太多，可以在这个区间内，再次使用桶排序来划分处理。桶排序比较适合用在外部排序中，比如数据保存在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。计数排序 Counting Sort 可以看作是桶排序的一种特殊情况。 假设8个考生，分数在0到5之间。这 8 个考生的成绩我们放在一个数组 A[8] 中，它们分别是：2，5，3，0，2，3，0，3。若按成绩排序，使用大小是6的数组C[6]表示桶，其中下标对应分数。不过C[6]内保存的不是考生，而是对应分数的考生个数。只需要遍历一次考生分数，就可以得到C[6]。假设上面3分的考生有3个，小于3分的考生有4个，那么成绩为3分的考生再排序后的数组R[8]中，会保存在下标4、5、6的位置。 下面的方法快速计算出最终排序后的数组：我们对 C[6] 数组顺序求和，C[6] 存储的数据就变成了下面这样子。C[k] 里存储小于等于分数 k 的考生个数。 我们从后到前依次扫描数组 A。比如，当扫描到 3 时，我们可以从数组 C 中取出下标为 3 的值 7，也就是说，到目前为止，包括自己在内，分数小于等于 3 的考生有 7 个，也就是说 3 是数组 R 中的第 7 个元素（也就是数组 R 中下标为 6 的位置）。当 3 放入到数组 R 中后，小于等于 3 的元素就只剩下了 6 个了，所以相应的 C[3] 要减 1，变成 6。 计数排序只能用在数据范围不大的场景中，如果数据范围 k 比要排序的数据 n 大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。基数排序 Radix Sort如果我们排序10w个手机号码，或者排序字典中的所有单词，可以用快排，时间复杂度到O(nlogn)。但不能用桶排序或者计数排序，因为手机号码11位，范围太大。针对这类排序，可以使用基数排序，时间复杂度位O(n)。 原理：可以从先排序最后一位，再依次排序前一位。注意这个每一位的排序算法要是稳定的。每一位的排序可以用桶排序或者计数排序。排序英文单词时，由于长度不同，可以把所有的单词都补齐到相同长度，位数不够的可以在后门补”0”。图图的基本概念：无向图、有向图、带权图、顶点、边、度、入度、出度。场景：微博：有向图，可以单方向关注。微信：无向图，只能双向关注。QQ的好友亲密度：带权图。存储邻接矩阵 Adjacency Matrix邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j] 和 A[j][i] 标记为 1；对于有向图来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j] 标记为 1。同理，如果有一条箭头从顶点 j 指向顶点 i 的边，我们就将 A[j][i] 标记为 1。对于带权图，数组中就存储相应的权重。邻接矩阵存储方法的缺点是比较浪费空间，但是优点是查询效率高，而且方便矩阵运算。邻接表 Adjacency List邻接表存储方法中每个顶点都对应一个链表，存储与其相连接的其他顶点。尽管邻接表的存储方式比较节省存储空间，但链表不方便查找，所以查询效率没有邻接矩阵存储方式高。针对这个问题，邻接表还有改进升级版，即将链表换成更加高效的动态数据结构，比如平衡二叉查找树、跳表、散列表等。邻接表的搜索public class Graph { // 无向图 private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } } public void addEdge(int s, int t) { // 无向图一条边存两次 adj[s].add(t); adj[t].add(s); }}广度优先搜索 Breadth-First-Search借助一个队列！三个重要的辅助变量visited, queue, prev： visited 是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点 q 被访问，那相应的 visited[q] 会被设置为 true。 queue 是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。 prev 用来记录搜索路径。这个路径是反向存储的。prev[w] 存储的是，顶点 w 是从哪个前驱顶点遍历过来的。比如，我们通过顶点 2 的邻接表访问到顶点 3，那 prev[3] 就等于 2。public void bfs(int s, int t) { if (s == t) return; boolean[] visited = new boolean[v]; visited[s]=true; Queue&amp;lt;Integer&amp;gt; queue = new LinkedList&amp;lt;&amp;gt;(); queue.add(s); int[] prev = new int[v]; for (int i = 0; i &amp;lt; v; ++i) { prev[i] = -1; } while (queue.size() != 0) { int w = queue.poll(); for (int i = 0; i &amp;lt; adj[w].size(); ++i) { int q = adj[w].get(i); if (!visited[q]) { prev[q] = w; if (q == t) { print(prev, s, t); return; } visited[q] = true; queue.add(q); } } }}private void print(int[] prev, int s, int t) { // 递归打印s-&amp;gt;t的路径 if (prev[t] != -1 &amp;amp;&amp;amp; t != s) { print(prev, s, prev[t]); } System.out.print(t + &quot; &quot;);}深度优先搜索 Depth-First-Search借助一个栈！深度优先搜索用的是一种比较著名的算法思想，回溯思想。这种思想解决问题的过程，非常适合用递归来实现。也需要用到 visited, prev 变量。boolean found = false; // 全局变量或者类成员变量public void dfs(int s, int t) { found = false; boolean[] visited = new boolean[v]; int[] prev = new int[v]; for (int i = 0; i &amp;lt; v; ++i) { prev[i] = -1; } recurDfs(s, t, visited, prev); print(prev, s, t);}private void recurDfs(int w, int t, boolean[] visited, int[] prev) { if (found == true) return; visited[w] = true; if (w == t) { found = true; return; } for (int i = 0; i &amp;lt; adj[w].size(); ++i) { int q = adj[w].get(i); if (!visited[q]) { prev[q] = w; recurDfs(q, t, visited, prev); } }}字符串匹配BF算法 Brute Force概念：主串，模式串。比方说，我们在字符串 A 中查找字符串 B，那字符串 A 就是主串，字符串 B 就是模式串。我们把主串的长度记作 n，模式串的长度记作 m。BF 算法的思想可以用一句话来概括，那就是，我们在主串中，检查起始位置分别是 0、1、2…n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的。尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。RK算法 Rabin-KarpBF算法的优化：将子串和模式串的字符串比较，转换成了哈希值的比较。这里的哈希值是将字符串转换成数字。比如字符串只出现a-z 26个字母，那么bca可以转换为1x26x26+2x26+0x26。26的阶乘可以提前计算出来提高速度。BM算法 Boyer-MooreBM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。坏字符规则，匹配字符串的时候从右往左匹配。发现不一致字符时，再看不一致字符在不在模式串中，如果不在，直接将模式串移动到主串的该字符后。好后缀的处理规则中最核心的内容：在模式串中，查找跟好后缀匹配的另一个子串；在好后缀的后缀子串中，查找最长的、能跟模式串前缀子串匹配的后缀子串。KMP算法KMP 算法对 BF 算法进行改进，引入了 next 数组，让匹配失败时，尽可能将模式串往后多滑动几位。重点是理解PMT数组：PMT中的值是字符串的前缀集合与后缀集合的交集中最长元素的长度。时间复杂度是O(m+n)，空间复杂度是O(m)。Trie树场景：搜索引擎的搜索关键词提示。Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。Trie 树占用内存太多。Trie 树不适合精确匹配查找，这种问题更适合用散列表或者红黑树来解决。Trie 树比较适合的是查找前缀匹配的字符串。AC自动机 Aho-Corasick算法场景：敏感词过滤。AC 自动机实际上就是在 Trie 树之上，加了类似 KMP 的 next 数组，只不过此处的 next 数组是构建在树上罢了。查找二分查找 时间复杂度是O(logn)，42亿个数中使用二分查找一个数据，最多需要比较32次。public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = (low + high) / 2; if (a[mid] == value) { return mid; } else if (a[mid] &amp;lt; value) { low = mid + 1; } else { high = mid - 1; } } return -1;}注意的点： 循环退出条件low&amp;lt;=high,不是low&amp;lt;high mid的取值mid=(low+high)/2是有问题的。如果low和high比较大，两者和就可能溢出。改进方法：mid=low+(high-low)/2，也就是mid=low+((high-low)»1)。 low和high的更新如果是low=mid或high=mid，可能发生死循环。比如，low=high=3，但是3这个位置不是查找的数据。// 二分查找的递归实现public int bsearch(int[] a, int n, int val) { return bsearchInternally(a, 0, n - 1, val);}private int bsearchInternally(int[] a, int low, int high, int value) { if (low &amp;gt; high) return -1; int mid = low + ((high - low) &amp;gt;&amp;gt; 1); if (a[mid] == value) { return mid; } else if (a[mid] &amp;lt; value) { return bsearchInternally(a, mid+1, high, value); } else { return bsearchInternally(a, low, mid-1, value); }}二分查找适用的场合： 保存在数组中 有序数据 数据量不能太大。因为数组需要连续的内存空间。 更适合静态数据，没有频繁的数据插入、删除操作，这样可以做一次排序O(nlogn)，然后使用二分O(logn)进行多次查找。对于有频繁数据插入、删除的操作，使用二叉树更好。 散列表和二叉树支持动态数据的查找，但是需要消耗数据以外的额外内存空间。二叉树底层依赖数组，最省内存空间。二分查找在查找某一值附近的边界值时，比散列表和二叉树更有优势。比如查找最后一个小于等于给定值的元素。二分查找算法的变体 查找第一个等于给定元素的值 查找最后一个等于给定元素的值 查找第一个大于等于给定元素的值 查找第一个小于等于给定元素的值// 查找第一个等于给定元素的值public int bsearch(int[] a, int n, int value) { int low = 0; int high = n - 1; while (low &amp;lt;= high) { int mid = low + ((high - low) &amp;gt;&amp;gt; 1); if (a[mid] &amp;gt; value) { high = mid - 1; } else if (a[mid] &amp;lt; value) { low = mid + 1; } else { if ((mid == 0) || (a[mid - 1] != value)) return mid; else high = mid - 1; } } return -1;}跳表 Skip List 为一个值有序的链表建立多级索引，比如每2个节点提取一个节点到上一级，我们把抽出来的那一级叫做索引或索引层。像这种为链表建立多级索引的数据结构就称为跳表。 Redis使用跳表来实现有序集合。 跳表使用空间换时间的设计思路，通过构建多级索引来提高查询的效率，实现了基于链表的“二分查找”。跳表是一种动态数据结构，支持快速的插入、删除、查找操作，每两个节点抽出一个做为上级节点时，时间复杂度都是 O(logn)。跳表的空间复杂度是 O(n)。跳表支持动态的插入、删除操作，而且插入、删除操作的时间复杂度也是 O(logn)。 当我们不停地往跳表中插入数据时，如果我们不更新索引，就有可能出现某 2 个索引结点之间数据非常多的情况。极端情况下，跳表还会退化成单链表。解决方法：当我们往跳表中插入数据的时候，我们可以选择同时将这个数据插入到部分索引层中。通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值 K，那我们就将这个结点添加到第一级到第 K 级这 K 级索引中。 跳表的实现非常灵活，可以通过改变索引构建策略，有效平衡执行效率和内存消耗。实现起来比红黑树更简单。散列表 Hash table 散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。查询平均是O(1)。 散列冲突：再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。Java 中 LinkedHashMap 就采用了链表法解决冲突，ThreadLocalMap 是通过线性探测的开放寻址法来解决冲突。开放寻址法当我们往散列表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散列冲突的原因。开放寻址法的散列表储存在数组中，比起链表来说，冲突的代价更高，更浪费内存。但这种方式序列化起来比较简单。装载因子只能小于1，当接近1时，就会有大量的散列冲突，导致再散列，性能会下降很多。链表法在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表。Java 中的 LinkedHashMap 就采用了链表法解决冲突。最差的查找时间是O(logn)。 散列表的装载因子 load factor散列表的装载因子=填入表中的元素个数/散列表的长度装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。当散列表的装载因子超过某个阈值时，就需要进行扩容。装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。装载因子阈值的设置要权衡时间、空间复杂度。如果内存空间不紧张，对执行效率要求很高，可以降低负载因子的阈值；相反，如果内存空间紧张，对执行效率要求又不高，可以增加负载因子的值，甚至可以大于 1。扩容方案如果一次就完成扩容，需要一次性将原有散列表的数据重新计算哈希值，会很耗时。所以扩容可以分批完成，当装载因子触达阈值之后，我们只申请新空间，但并不将老的数据搬移到新散列表中。当有新数据要插入时，我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表。每次插入一个数据到散列表，我们都重复上面的过程。对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。这种实现方式，任何情况下，插入一个数据的时间复杂度都是 O(1)。应用 在 JDK1.8 版本中，为了对 HashMap 做进一步优化，我们引入了红黑树。而当链表长度太长（默认超过 8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高 HashMap 的性能。当红黑树结点个数少于 8 个的时候，又会将红黑树转化为链表。因为在数据量较小的情况下，红黑树要维护平衡，比起链表来，性能上的优势并不明显。 LRU：如果只是一个单链表，那么时间复杂度是O(n)。但是可以通过散列表和双向链表结合，来达到O(1)的时间复杂度。散列表用O(1)可以找到需要的数据，双向链表可以使数据的增删也变成O(1)。 Java的 LinkedHashMap 就采用上面LRU的实现算法，这里的Linked并不是说用链表解决散列冲突，而是用链表实现了数据插入和访问的顺序，新put和新get的数据都在链表尾。 Redis有序集合Redis有序集合里面有key（键值）和score（分值）。按key将数据构建成一个散列表，按score构建跳表，来保证用key检索或用score区间检索的高效。 位运算基础知识 &amp;amp; 与， 或， ^ 非 左移 «。左边的将被丢弃，右边补0。 右移 »。两种情况： 无符号数，左侧补0 右符号数，左侧补0或者1，根据最左侧的位来决定 技巧 一个整数减去1，再和原数做与运算，会把改整数最后边的一个1变成0。计算一个整数的二进制有多少个1检查下面代码中的错误int GetOneCount(int n){ int count = 0; int bit = 1; for (int i = 0; i &amp;lt; sizeof(int)*8; i++) { if (bit&amp;amp;n &amp;gt; 0) count++; bit = bit &amp;lt;&amp;lt; 1; } return count;}上面第一次写出来，错误的地方有： &amp;amp;的优先级比&amp;gt;低，所以应该是 (bit&amp;amp;n)&amp;gt;0 bit是int的，有符号，如果n是负数，与运算后首位是1，还是负数，不能用&amp;gt;0来判断改正后：int GetOneCount(int n){ int count = 0; unsigned int flag = 1; while(flag) { if (n&amp;amp;flag) count++; flag = flag &amp;lt;&amp;lt; 1; } return count;}优化的做法，有几个1循环几次：int GetOneCount(int n){ int count = 0; while(n) { count++; n = (n-1)&amp;amp;n; } return count;}测试需要覆盖到： 正数（包括1，0x7FFFFFFF) 负数（包括0x80000000，0xFFFFFFFF) 0布隆过滤器 Bloom Filter本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。原始的位图：public class BitMap { // Java中char类型占16bit，也即是2个字节 private char[] bytes; private int nbits; public BitMap(int nbits) { this.nbits = nbits; this.bytes = new char[nbits/16+1]; } public void set(int k) { if (k &amp;gt; nbits) return; int byteIndex = k / 16; int bitIndex = k % 16; bytes[byteIndex] |= (1 &amp;lt;&amp;lt; bitIndex); } public boolean get(int k) { if (k &amp;gt; nbits) return false; int byteIndex = k / 16; int bitIndex = k % 16; return (bytes[byteIndex] &amp;amp; (1 &amp;lt;&amp;lt; bitIndex)) != 0; }}如果用位图，数据的范围很大的时候就不方便。比如数据个数是 1 千万，数据的范围是 1 到 10 亿，用位图就需要能储存10亿的位图。布隆过滤器的做法是，我们使用一个 1 亿个二进制大小的位图，然后通过哈希函数，对数字进行处理，让它落在这 1 到 1 亿范围内。比如我们把哈希函数设计成 f(x)=x%n。其中，x 表示数字，n 表示位图的大小（1 亿），也就是，对数字跟位图的大小进行取模求余。这样会产生冲突，为了减少冲突，就用多个不同的hash函数，同时用多个位来确定一个数。 布隆过滤器非常适合这种不需要 100% 准确的、允许存在小概率误判的大规模判重场景。 Java 中的 BitSet 类就是一个位图，Redis 也提供了 BitMap 位图类，Google 的 Guava 工具包提供了 BloomFilter 布隆过滤器的实现。二叉树分类Linked list就是特殊化的Tree，Tree就是特殊化的Graph。 binary tree. 二叉树 binary search tree. (alias: ordered binary tree, sorted binary tree) 空树，或满足下面的特点：左子树上所有节点的值都小于它的根节点的值，右子树上所有节点的值都大于它的根节点的值，Recursively，左右子树也分别为二叉查找树。 红黑树是一种特殊的二叉搜索树，最坏情况下的O效率更高定义 高度 Height, 深度 Depth，层 Level 编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。 编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。链式存储 和 顺序存储 链式存储：比较常见，每个节点还有左右两个指针，分别指向左子节点和右子节点。顺序存储：基于数组。如果节点 X 存储在数组中下标为 i 的位置，下标为 2 * i 的位置存储的就是左子节点，下标为 2 * i + 1 的位置存储的就是右子节点。反过来，下标为 i/2 的位置存储就是它的父节点。但是如果是非完全二叉树，就会浪费存储空间。二叉搜索树 查找一个节点 我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。 添加一个节点 二叉查找树的插入过程有点类似查找操作。新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。同理，如果要插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空，就再递归遍历左子树，查找插入位置。 删除一个节点 二叉查找树的查找、插入操作都比较简单易懂，但是它的删除操作就比较复杂了 。针对要删除节点的子节点个数的不同，我们需要分三种情况来处理。 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为 null。比如图中的删除节点 55。 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点 13。 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点。比如图中的删除节点 18。public void delete(int data) { Node p = tree; // p指向要删除的节点，初始化指向根节点 Node pp = null; // pp记录的是p的父节点 while (p != null &amp;amp;&amp;amp; p.data != data) { pp = p; if (data &amp;gt; p.data) p = p.right; else p = p.left; } if (p == null) return; // 没有找到 // 要删除的节点有两个子节点 if (p.left != null &amp;amp;&amp;amp; p.right != null) { // 查找右子树中最小节点 Node minP = p.right; Node minPP = p; // minPP表示minP的父节点 while (minP.left != null) { minPP = minP; minP = minP.left; } p.data = minP.data; // 将minP的数据替换到p中 p = minP; // 下面就变成了删除minP了 pp = minPP; } // 删除节点是叶子节点或者仅有一个子节点 Node child; // p的子节点 if (p.left != null) child = p.left; else if (p.right != null) child = p.right; else child = null; if (pp == null) tree = child; // 删除的是根节点 else if (pp.left == p) pp.left = child; else pp.right = child;}红黑树(平衡二叉搜索树) Red-Black Tree 红黑树的起源：前面的二叉查找树是最常用的一种二叉树，它支持快速插入、删除、查找操作，各个操作的时间复杂度跟树的高度成正比，理想情况下，时间复杂度是 O(logn)。不过，二叉查找树在频繁的动态更新过程中，可能会出现树的高度远大于 log2n 的情况，从而导致各个操作的效率下降。极端情况下，二叉树会退化为链表，时间复杂度会退化到 O(n)。所以有了平衡二叉查找树。定义：红黑树中的节点，一类被标记为黑色，一类被标记为红色。除此之外，一棵红黑树还需要满足这样几个要求： 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；验证二叉搜索树 中序把树遍历出来，然后去重、排序后进行比较。# 代码看起来简单，但是list(sorted(set()))效率很低def isValidBST(self, root): inorder = self.inorder(root) return inorder == list(sorted(set(inorder))) def inorder(self, root): if root is None: return [] return self.inorder(root.left) + [root.val] + self.inorder(root.right) 改进，效率更高：def isValidBST(self, root): self.prev = None return self.helper(root) def helper(self, root): if root is None: return True if not self.helper(root.left): return False if self.prev and self.prev.val &amp;gt;= root.val: return False self.prev = root return self.helper(root.right) 一种简单的遍历，每个节点都应该在一个值的范围内def isValidBST(root, min, max): if root is not None: return True if min != None &amp;amp;&amp;amp; root.val &amp;lt;= min: return False if max != None &amp;amp;&amp;amp; root.val &amp;gt;= max: return False return isValidBST(root.left, min, root.val) &amp;amp;&amp;amp; isValidBST(root.right, root.val, max)二叉树的最小公共祖先给出两个节点，找到他们的最小公共祖先。 遍历。分别遍历左右子树，如果一边找到另外一边没找到，就返回找到的一边。如果两边都找到，就说明当前节点就是最小公共祖先。TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q)TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); return left == null ? right : right == null ? left : root;}如果是二叉搜索树，则可以更简单。 递归。如果当前节点&amp;gt;p和q，那么就往右搜索。反之，往左搜索。def lowestCommonAncestor(root, p, q): if root.val &amp;gt; p.val &amp;amp;&amp;amp; root.val &amp;gt; q.val: return lowestCommonAncestor(root.left, p, q) if root.val &amp;lt; p.val &amp;amp;&amp;amp; root.val &amp;lt; q.val: return lowestCommonAncestor(root.right, p, q) return root 不用递归。def lowestCommonAncestor(root, p, q): while root: if root.val &amp;gt; p.val &amp;amp;&amp;amp; root.val &amp;gt; q.val: root = root.left elif root.val &amp;lt; p.val &amp;amp;&amp;amp; root.val &amp;lt; q.val: root = root.right else: return root三种遍历按根所处的位置来分：（时间复杂度是O(n)） Pre-order 前序遍历(根，左，右) In-order 中序遍历(左，根，右) Post-order 后序遍历(左，右，根) 如果是普通的二叉树，这三种方式遍历的意义不大。如果是二叉搜索树，中序遍历出来是一个有序数组。def preorder(root): if root: traverse_path.append(root.val) preorder(root.left) preorder(root.right) def inorder(root): if root: inorder(root.left) traverse_path.append(root.val) inorder(root.right)def postorder(root): if root: postorder(root.left) postorder(root.right) traverse_path.append(root.val)散列表 vs 二叉树二叉树退化成链表的时候，查找的时间复杂度就退化成了O(n)。平衡二叉查找树的时间复杂度和树的高度成正比，是O(height)，也就是小于O(logn)。散列表的添加、删除、查找可以做到O(1)。但散列表也有劣势： 散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。 散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。 尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。 散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。 为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。AVL vs 红黑树AVL 树是一种高度平衡的二叉树，所以查找的效率非常高，但是，有利就有弊，AVL 树为了维持这种高度的平衡，就要付出更多的代价。每次插入、删除都要做调整，就比较复杂、耗时。所以，对于有频繁的插入、删除操作的数据集合，使用 AVL 树的代价就有点高了。红黑树只是做到了近似平衡，并不是严格的平衡，所以在维护平衡的成本上，要比 AVL 树要低。所以，红黑树的插入、删除、查找各种操作性能都比较稳定。对于工程应用来说，要面对各种异常情况，为了支撑这种工业级的应用，我们更倾向于这种性能稳定的平衡二叉查找树。红黑树是一种平衡二叉查找树。它是为了解决普通二叉查找树在数据更新的过程中，复杂度退化的问题而产生的。红黑树的高度近似 log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是 O(logn)。B+树B+树的特点： 每个节点中子节点的个数不能超过 m，也不能小于 m/2； 根节点的子节点个数可以不超过 m/2，这是一个例外； m 叉树只存储索引，并不真正存储数据，这个有点儿类似跳表； 通过链表将叶子节点串联在一起，这样可以方便按区间查找； 一般情况，根节点会被存储在内存中，其他节点存储在磁盘中。B-树(B树)的特点： B+ 树中的节点不存储数据，只是索引，而 B 树中的节点存储数据； B 树中的叶子节点并不需要链表来串联。算法思想贪心算法 Greedy Algorithm例子：背包问题，最短路径，零钱问题。贪心算法不一定是最优解。分治 Divide and Conquer分而治之 ，也就是将原问题划分成 n 个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果，就得到原问题的解。分治算法的递归实现中，每一层递归都会涉及这样三个操作： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。回溯算法回溯算法的思想非常简单，大部分情况下，都是用来解决广义的搜索问题，也就是，从一组可能的解中，选择出一个满足要求的解。回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。尽管回溯算法的原理非常简单，但是却可以解决很多问题，比如深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配等等。进入回溯先检查退出条件，再看是否进入下一个回溯。八皇后：int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列public void cal8queens(int row) { // 调用方式：cal8queens(0); if (row == 8) { // 8个棋子都放置好了，打印结果 printQueens(result); return; // 8行棋子都放好了，已经没法再往下递归了，所以就return } for (int column = 0; column &amp;lt; 8; ++column) { // 每一行都有8中放法 if (isOk(row, column)) { // 有些放法不满足要求 result[row] = column; // 第row行的棋子放到了column列 cal8queens(row+1); // 考察下一行 } }}private boolean isOk(int row, int column) {//判断row行column列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &amp;gt;= 0; --i) { // 逐行往上考察每一行 if (result[i] == column) return false; // 第i行的column列有棋子吗？ if (leftup &amp;gt;= 0) { // 考察左上对角线：第i行leftup列有棋子吗？ if (result[i] == leftup) return false; } if (rightup &amp;lt; 8) { // 考察右上对角线：第i行rightup列有棋子吗？ if (result[i] == rightup) return false; } --leftup; ++rightup; } return true;}private void printQueens(int[] result) { // 打印出一个二维矩阵 for (int row = 0; row &amp;lt; 8; ++row) { for (int column = 0; column &amp;lt; 8; ++column) { if (result[row] == column) System.out.print(&quot;Q &quot;); else System.out.print(&quot;* &quot;); } System.out.println(); } System.out.println();}动态规划 Dynamic Programming 模型：多阶段决策最优解模型。三个特征： 最优子结构，问题的最优解包含子问题的最优解。 无后效性，第一层含义是，在推导后面阶段的状态的时候，我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的。第二层含义是，某阶段状态一旦确定，就不受之后阶段的决策影响。 重复子问题，不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态。大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的。动态规划算法，在执行效率方面，要高很多。尽管执行效率提高了，但是动态规划的空间复杂度也提高了，所以，动态规划是一种空间换时间的算法思想。0-1背包问题有一个最多能装重量是w的背包，有多种重量的物品items，问w背包中装items最多能装的重量是多少？创建一个一维数组，每个物品都有放和不放两种情况，把每个物品放和不放，所占用的重量都列出来，然后就知道背包能装的最多东西的重量是多少了。public static int knapsack2(int[] items, int n, int w) { boolean[] states = new boolean[w+1]; // 默认值false states[0] = true; // 第一行的数据要特殊处理，可以利用哨兵优化 if (items[0] &amp;lt;= w) { states[items[0]] = true; } for (int i = 1; i &amp;lt; n; ++i) { // 动态规划 for (int j = w-items[i]; j &amp;gt;= 0; --j) {//把第i个物品放入背包 if (states[j]==true) states[j+items[i]] = true; } } for (int i = w; i &amp;gt;= 0; --i) { // 输出结果 if (states[i] == true) return i; } return 0;}升级版本： 刚才只涉及背包重量和物品重量，我们现在引入物品价值这一变量。对于一组不同重量、不同价值、不可分割的物品，我们选择将某些物品装入背包，在满足背包最大重量限制的前提下，背包中可装入物品的总价值最大是多少呢？用回溯算法：private int maxV = Integer.MIN_VALUE; // 结果放到maxV中private int[] items = {2，2，4，6，3}; // 物品的重量private int[] value = {3，4，8，9，6}; // 物品的价值private int n = 5; // 物品个数private int w = 9; // 背包承受的最大重量public void finditems(int i, int cw, int cv) { // 调用f(0, 0, 0) if (cw == w || i == n) { // cw==w表示装满了，i==n表示物品都考察完了 if (cv &amp;gt; maxV) maxV = cv; return; } finditems(i+1, cw, cv); // 选择不装第i个物品 if (cw + weight[i] &amp;lt;= w) { finditems(i+1,cw+weight[i], cv+value[i]); // 选择装第i个物品 }}动态规划：还是把整个求解过程分为 n 个阶段，每个阶段会决策一个物品是否放到背包中。每个阶段决策完之后，背包中的物品的总重量以及总价值，会有多种情况，也就是会达到多种不同的状态。用一个二维数组 states[n][w+1]，来记录每层可以达到的不同状态。不过这里数组存储的值不再是 boolean 类型的了，而是当前状态对应的最大总价值。我们把每一层中 (i, cw) 重复的状态（节点）合并，只记录 cv 值最大的那个状态，然后基于这些状态来推导下一层的状态。时间复杂度是 O(n x w)，空间复杂度也是 O(n x w)。public static int knapsack3(int[] weight, int[] value, int n, int w) { int[][] states = new int[n][w+1]; for (int i = 0; i &amp;lt; n; ++i) { // 初始化states for (int j = 0; j &amp;lt; w+1; ++j) { states[i][j] = -1; } } states[0][0] = 0; if (weight[0] &amp;lt;= w) { states[0][weight[0]] = value[0]; } for (int i = 1; i &amp;lt; n; ++i) { //动态规划，状态转移 for (int j = 0; j &amp;lt;= w; ++j) { // 不选择第i个物品 if (states[i-1][j] &amp;gt;= 0) states[i][j] = states[i-1][j]; } for (int j = 0; j &amp;lt;= w-weight[i]; ++j) { // 选择第i个物品 if (states[i-1][j] &amp;gt;= 0) { int v = states[i-1][j] + value[i]; if (v &amp;gt; states[i][j+weight[i]]) { states[i][j+weight[i]] = v; } } } } // 找出最大值 int maxvalue = -1; for (int j = 0; j &amp;lt;= w; ++j) { if (states[n-1][j] &amp;gt; maxvalue) maxvalue = states[n-1][j]; } return maxvalue;}两种动态规划的解题思路假设我们有一个 n 乘以 n 的矩阵 w[n][n]。矩阵存储的都是正整数。棋子起始位置在左上角，终止位置在右下角。我们将棋子从左上角移动到右下角。每次只能向右或者向下移动一位。从左上角到右下角，会有很多不同的路径可以走。我们把每条路径经过的数字加起来看作路径的长度。那从左上角移动到右下角的最短路径长度是多少呢？用回溯的方法实现：private int minDist = Integer.MAX_VALUE; // 全局变量或者成员变量// 调用方式：minDistBacktracing(0, 0, 0, w, n);public void minDistBT(int i, int j, int dist, int[][] w, int n) { // 到达了n-1, n-1这个位置了，这里看着有点奇怪哈，你自己举个例子看下 if (i == n &amp;amp;&amp;amp; j == n) { if (dist &amp;lt; minDist) minDist = dist; return; } if (i &amp;lt; n) { // 往下走，更新i=i+1, j=j minDistBT(i + 1, j, dist+w[i][j], w, n); } if (j &amp;lt; n) { // 往右走，更新i=i, j=j+1 minDistBT(i, j+1, dist+w[i][j], w, n); }} 状态转移表法先画出一个状态表。状态表一般都是二维的，所以你可以把它想象成二维数组。其中，每个状态包含三个变量，行、列、数组值。我们根据决策的先后过程，从前往后，根据递推关系，分阶段填充状态表中的每个状态。最后，我们将这个递推填表的过程，翻译成代码，就是动态规划代码了。public int minDistDP(int[][] matrix, int n) { int[][] states = new int[n][n]; int sum = 0; for (int j = 0; j &amp;lt; n; ++j) { // 初始化states的第一行数据 sum += matrix[0][j]; states[0][j] = sum; } sum = 0; for (int i = 0; i &amp;lt; n; ++i) { // 初始化states的第一列数据 sum += matrix[i][0]; states[i][0] = sum; } for (int i = 1; i &amp;lt; n; ++i) { for (int j = 1; j &amp;lt; n; ++j) { states[i][j] = matrix[i][j] + Math.min(states[i][j-1], states[i-1][j]); } } return states[n-1][n-1];} 状态转移方程法：有点类似递归的解题思路。我们需要分析，某个问题如何通过子问题来递归求解，也就是所谓的最优子结构。根据最优子结构，写出递归公式，也就是所谓的状态转移方程。有了状态转移方程，代码实现就非常简单了。一般情况下，我们有两种代码实现方法，一种是递归加“备忘录”，另一种是迭代递推。状态转移方程min_dist(i, j) = w[i][j] + min(min_dist(i, j-1), min_dist(i-1, j))代码实现：private int[][] matrix = { {1，3，5，9}, {2，1，3，4}，{5，2，6，7}，{6，8，4，3} };private int n = 4;private int[][] mem = new int[4][4];public int minDist(int i, int j) { // 调用minDist(n-1, n-1); if (i == 0 &amp;amp;&amp;amp; j == 0) return matrix[0][0]; if (mem[i][j] &amp;gt; 0) return mem[i][j]; int minLeft = Integer.MAX_VALUE; if (j-1 &amp;gt;= 0) { minLeft = minDist(i, j-1); } int minUp = Integer.MAX_VALUE; if (i-1 &amp;gt;= 0) { minUp = minDist(i-1, j); } int currMinDist = matrix[i][j] + Math.min(minLeft, minUp); mem[i][j] = currMinDist; return currMinDist;}递归 Recursion递归的特点 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件，即必须有一个明确的递归结束条件，称之为递归出口 问题：警惕堆栈溢出。求解思路最关键的是： 写出递推公式，找到终止条件。写递归代码就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。 例子：有n个台阶，每次可以上1个或者2个台阶，有多少种走法？分析：到第n个台阶的走法数，其实是到n-1个台阶的走法数(再上一个台阶)+n-2个台阶的走法数(再上两个台阶)所以递推公式就是：f(n)=f(n-1)+f(n-2)终止条件是：f(1)=1, f(2)=2int f(int n) { if (n == 1) return 1; if (n == 2) return 2; return f(n-1) + f(n-2);} 如果一个问题 A 可以分解为 B、C、D，你可以假设子问题B、C、D已经解决，在此基础上思考如何解决问题 A。而且，你只需要思考问题A和子问题B、C、D两层之间的关系即可，不需要一层层往下思考子问题和子子问题，子问题和子子问题之间的关系。屏蔽掉递归细节，理解起来就简单多了。递归的问题以及改进 警惕堆栈溢出函数递归调用可能导致堆栈溢出，可以用变量来保存递归深度，超过该深度就返回，避免溢出。 警惕重复计算比如前面的例子中，f(5)需要计算f(4)和f(3)，f(4)需要计算f(3)和f(2)，其中 f(3) 被重复计算。上面例子里的代码可改造为：public int f(int n) { if (n == 1) return 1; if (n == 2) return 2; // hasSolvedList可以理解成一个Map，key是n，value是f(n) if (hasSolvedList.containsKey(n)) { return hasSolvedList.get(n); } int ret = f(n-1) + f(n-2); hasSolvedList.put(n, ret); return ret;}递归的框架注意递归有时有重复计算的情况，效率会降低。def recursion(level, param1, param2, ...): # recursion terminator if level &amp;gt; MAX_LEVEL: print_result return # process logic in current level process_data(level, data...) # drill down recursion(level+1, p1, ...) # reverse the current level status if needed reverse_state(level)分治 Divide &amp;amp; Conquer 如果分解的各个子问题可以独立运算，那么就可以用分治的方法。分治通常也用递归来实现。def divide_conquer(problem, param1, param2, ...): # recursion terminator if problem is None: print_result return # prepare data data = prepare_data(problem) subproblems = split_problem(problem, data) # conquer subproblems subresult1 = divide_conquer(subproblems[0], p1, ...) subresult2 = divide_conquer(subproblems[1], p1, ...) subresult3 = divide_conquer(subproblems[2], p1, ...) ... # process and generate the final subresult result = process_result(subresult1, subresult2, subresult3, ...)计算x的n次方 采用分治的方法：从中间分开，两边乘，避免了重复运算。注意n是偶数和奇数的情况。# 递归的方法def myPow(x, n): if not n: return 1 if n &amp;lt; 0: return 1 / myPow(x, -n) if n % 2: return x * myPow(x, n-1) return myPow(x*x, n/2) # 非递归的方法def myPow(x, n): if n &amp;lt; 0: x = 1/x n = -n pow = 1 while n: if n &amp;amp; 1: pow *= x x *= x n &amp;gt;&amp;gt;= 1 return pow其它拓扑排序比如编译文件时，文件间的相互依赖关系。拓扑排序的结果并不是唯一的。拓扑排序里面不能出现环，所以是一个有向无环图。public class Graph { private int v; // 顶点的个数 private LinkedList&amp;lt;Integer&amp;gt; adj[]; // 邻接表 public Graph(int v) { this.v = v; adj = new LinkedList[v]; for (int i=0; i&amp;lt;v; ++i) { adj[i] = new LinkedList&amp;lt;&amp;gt;(); } } public void addEdge(int s, int t) { // s先于t，边s-&amp;gt;t adj[s].add(t); }} Kahn算法Kahn 算法实际上用的是贪心算法思想。定义数据结构的时候，如果 s 需要先于 t 执行，那就添加一条 s 指向 t 的边。所以，如果某个顶点入度为 0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。我们先从图中，找出一个入度为 0 的顶点，将其输出到拓扑排序的结果序列中（对应代码中就是把它打印出来），并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减 1）。我们循环执行上面的过程，直到所有的顶点都被输出。最后输出的序列，就是满足局部依赖关系的拓扑排序。public void topoSortByKahn() { int[] inDegree = new int[v]; // 统计每个顶点的入度 for (int i = 0; i &amp;lt; v; ++i) { for (int j = 0; j &amp;lt; adj[i].size(); ++j) { int w = adj[i].get(j); // i-&amp;gt;w inDegree[w]++; } } LinkedList&amp;lt;Integer&amp;gt; queue = new LinkedList&amp;lt;&amp;gt;(); for (int i = 0; i &amp;lt; v; ++i) { if (inDegree[i] == 0) queue.add(i); } while (!queue.isEmpty()) { int i = queue.remove(); System.out.print(&quot;-&amp;gt;&quot; + i); for (int j = 0; j &amp;lt; adj[i].size(); ++j) { int k = adj[i].get(j); inDegree[k]--; if (inDegree[k] == 0) queue.add(k); } }} DFS 算法这个算法包含两个关键部分。第一部分是通过邻接表构造逆邻接表。邻接表中，边 s-&amp;gt;t 表示 s 先于 t 执行，也就是 t 要依赖 s。在逆邻接表中，边 s-&amp;gt;t 表示 s 依赖于 t，s 后于 t 执行。为什么这么转化呢？这个跟我们这个算法的实现思想有关。第二部分是这个算法的核心，也就是递归处理每个顶点。对于顶点 vertex 来说，我们先输出它可达的所有顶点，也就是说，先把它依赖的所有的顶点输出了，然后再输出自己。public void topoSortByDFS() { // 先构建逆邻接表，边s-&amp;gt;t表示，s依赖于t，t先于s LinkedList&amp;lt;Integer&amp;gt; inverseAdj[] = new LinkedList[v]; for (int i = 0; i &amp;lt; v; ++i) { // 申请空间 inverseAdj[i] = new LinkedList&amp;lt;&amp;gt;(); } for (int i = 0; i &amp;lt; v; ++i) { // 通过邻接表生成逆邻接表 for (int j = 0; j &amp;lt; adj[i].size(); ++j) { int w = adj[i].get(j); // i-&amp;gt;w inverseAdj[w].add(i); // w-&amp;gt;i } } boolean[] visited = new boolean[v]; for (int i = 0; i &amp;lt; v; ++i) { // 深度优先遍历图 if (visited[i] == false) { visited[i] = true; dfs(i, inverseAdj, visited); } }}private void dfs( int vertex, LinkedList&amp;lt;Integer&amp;gt; inverseAdj[], boolean[] visited) { for (int i = 0; i &amp;lt; inverseAdj[vertex].size(); ++i) { int w = inverseAdj[vertex].get(i); if (visited[w] == true) continue; visited[w] = true; dfs(w, inverseAdj, visited); } // 先把vertex这个顶点可达的所有顶点都打印出来之后，再打印它自己 System.out.print(&quot;-&amp;gt;&quot; + vertex);} 拓扑排序应用非常广泛，解决的问题的模型也非常一致。凡是需要通过局部顺序来推导全局顺序的，一般都能用拓扑排序来解决。除此之外，拓扑排序还能检测图中环的存在。对于 Kahn 算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是 0 的顶点，那就说明，图中存在环。最短路径（Dijkstra算法)把每个岔路口看作一个顶点，岔路口与岔路口之间的路看作一条边，路的长度就是边的权重。如果路是单行道，我们就在两个顶点之间画一条有向边；如果路是双行道，我们就在两个顶点之间画两条方向不同的边。这样，整个地图就被抽象成一个有向有权图。Dijkstra算法可以看作是动态规划，又有点像BFS，从起始点依次扩散出去，看周围各个顶点到起始点的最短距离。// 因为Java提供的优先级队列，没有暴露更新数据的接口，所以我们需要重新实现一个private class PriorityQueue { // 根据vertex.dist构建小顶堆 private Vertex[] nodes; private int count; public PriorityQueue(int v) { this.nodes = new Vertex[v+1]; this.count = v; } public Vertex poll() { // TODO: 留给读者实现... } public void add(Vertex vertex) { // TODO: 留给读者实现...} // 更新结点的值，并且从下往上堆化，重新符合堆的定义。时间复杂度O(logn)。 public void update(Vertex vertex) { // TODO: 留给读者实现...} public boolean isEmpty() { // TODO: 留给读者实现...}}public void dijkstra(int s, int t) { // 从顶点s到顶点t的最短路径 int[] predecessor = new int[this.v]; // 用来还原最短路径 Vertex[] vertexes = new Vertex[this.v]; for (int i = 0; i &amp;lt; this.v; ++i) { vertexes[i] = new Vertex(i, Integer.MAX_VALUE); } PriorityQueue queue = new PriorityQueue(this.v);// 小顶堆 boolean[] inqueue = new boolean[this.v]; // 标记是否进入过队列 vertexes[s].dist = 0; queue.add(vertexes[s]); inqueue[s] = true; while (!queue.isEmpty()) { Vertex minVertex= queue.poll(); // 取堆顶元素并删除 if (minVertex.id == t) break; // 最短路径产生了 for (int i = 0; i &amp;lt; adj[minVertex.id].size(); ++i) { Edge e = adj[minVertex.id].get(i); // 取出一条minVetex相连的边 Vertex nextVertex = vertexes[e.tid]; // minVertex--&amp;gt;nextVertex if (minVertex.dist + e.w &amp;lt; nextVertex.dist) { // 更新next的dist nextVertex.dist = minVertex.dist + e.w; predecessor[nextVertex.id] = minVertex.id; if (inqueue[nextVertex.id] == true) { queue.update(nextVertex); // 更新队列中的dist值 } else { queue.add(nextVertex); inqueue[nextVertex.id] = true; } } } } // 输出最短路径 System.out.print(s); print(s, t, predecessor);}private void print(int s, int t, int[] predecessor) { if (s == t) return; print(s, predecessor[t], predecessor); System.out.print(&quot;-&amp;gt;&quot; + t);}我们用 vertexes 数组，记录从起始顶点到每个顶点的距离（dist）。起初，我们把所有顶点的 dist 都初始化为无穷大（也就是代码中的 Integer.MAX_VALUE）。我们把起始顶点的 dist 值初始化为 0，然后将其放到优先级队列中。我们从优先级队列中取出 dist 最小的顶点 minVertex，然后考察这个顶点可达的所有顶点（代码中的 nextVertex）。如果 minVertex 的 dist 值加上 minVertex 与 nextVertex 之间边的权重 w 小于 nextVertex 当前的 dist 值，也就是说，存在另一条更短的路径，它经过 minVertex 到达 nextVertex。那我们就把 nextVertex 的 dist 更新为 minVertex 的 dist 值加上 w。然后，我们把 nextVertex 加入到优先级队列中。重复这个过程，直到找到终止顶点 t 或者队列为空。以上就是 Dijkstra 算法的核心逻辑。除此之外，代码中还有两个额外的变量，predecessor 数组和 inqueue 数组。predecessor 数组的作用是为了还原最短路径，它记录每个顶点的前驱顶点。最后，我们通过递归的方式，将这个路径打印出来。打印路径的 print 递归代码我就不详细讲了，这个跟我们在图的搜索中讲的打印路径方法一样。如果不理解的话，你可以回过头去看下那一节。inqueue 数组是为了避免将一个顶点多次添加到优先级队列中。我们更新了某个顶点的 dist 值之后，如果这个顶点已经在优先级队列中了，就不要再将它重复添加进去了。简易搜索引擎中使用的算法搜索引擎大致可以分为四个部分：搜集、分析、索引、查询。 搜集：就是我们常说的利用爬虫爬取网页。 分析：主要负责网页内容抽取、分词，构建临时索引，计算 PageRank 值这几部分工作。 索引：主要负责通过分析阶段得到的临时索引，构建倒排索引。 查询：主要负责响应用户的请求，根据倒排索引获取相关网页，计算网页排名，返回查询结果给用户。搜集 爬虫在爬取网页的过程中，涉及的四个重要的文件。其中，links.bin 和 bloom_filter.bin 这两个文件是爬虫自身所用的。另外的两个（doc_raw.bin、doc_id.bin）是作为搜集阶段的成果，供后面的分析、索引、查询用的。 待爬取网页链接文件：links.bin在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。于是，队列中的链接就会越来越多，可能会多到内存放不下。所以，我们用一个存储在磁盘中的文件（links.bin）来作为广度优先搜索中的队列。爬虫从 links.bin 文件中，取出链接去爬取对应的页面。等爬取到网页之后，将解析出来的链接，直接存储到 links.bin 文件中。断点续传。 网页判重文件：bloom_filter.bin使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。 原始网页存储文件：doc_raw.bin把多个网页存储在一个文件中。每个网页之间，通过一定的标识进行分隔，方便后续读取。比如：doc1_id \\t doc1_size \\t doc1 \\r\\n\\r\\n 网页链接及其编号的对应文件：doc_id.bin可以按照网页被爬取的先后顺序，从小到大依次编号。具体是这样做的：我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，我们将网页链接跟编号之间的对应关系，存储在另一个 doc_id.bin 文件中。 分析 抽取网页文本信息依靠 HTML 标签来抽取网页中的文本信息。这个抽取的过程，大体可以分为两步。第一步是去掉 JavaScript 代码、CSS 格式以及下拉框中的内容。第二步是去掉所有 HTML 标签。可以利用 AC 自动机这种多模式串匹配算法。 分词并创建临时索引：temp_index.bin对于英文网页来说，分词只需要通过空格、标点符号等分隔符，将每个单词分割开来就可以了。对于中文来说，可以基于字典和规则的分词方法。字典也叫词库，里面包含大量常用的词语。我们借助词库并采用最长匹配规则，来对文本进行分词。可以将词库中的单词，构建成Trie树。每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。比如：term1_id \\t doc_id \\r\\n 构建单词索引：term_id.bin给每个单词编号，在temp_index.bin中使用单词id，而不是单词本身，来减小文件大小。使用散列表根据单词来查id。 索引 索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。倒排索引文件(index.bin)：term_id \\t doc1_id,doc2_id,doc3_id,… \\r\\n临时索引文件很大，无法一次性加载到内存中，搜索引擎一般会选择使用多路归并排序的方法来实现。除了倒排文件之外，我们还需要一个文件(term_offset.bin)，来记录每个单词编号在倒排索引文件中的偏移位置：tid1 \\t offset1 \\r\\n查询前面几部准备好的文件有： doc_id.bin：记录网页链接和编号之间的对应关系 term_id.bin：记录单词和编号之间的对应关系。 index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。 term_offset.bin：记录每个单词编号在倒排索引文件中的偏移位置。 这四个文件中，除了倒排索引文件（index.bin）比较大之外，其他的都比较小。 k个单词 -&amp;gt; 找term id(term_id.bin) -&amp;gt; 找offset(term_offset.bin) -&amp;gt; 找网页列表(index.bin) -&amp;gt; 统计每个网页编号出现的次数，排序 -&amp;gt; 找网页连接(doc_id.bin)练习Leetcode 相关题目 链表206，141，21，19，876 栈20, 155, 232, 844, 224, 682, 496思考题 假设我们有 10 万条 URL 访问日志，如何按照访问次数给 URL 排序？ 遍历 10 万条数据，以 URL 为 key，访问次数为 value，存入散列表，同时记录下访问次数的最大值 K，时间复杂度 O(N)。如果 K 不是很大，可以使用桶排序，时间复杂度 O(N)。如果 K 非常大（比如大于 10 万），就使用快速排序，复杂度 O(NlogN)。 有两个字符串数组，每个数组大约有 10 万条字符串，如何快速找出两个数组中相同的字符串？ 以第一个字符串数组构建散列表，key 为字符串，value 为出现次数。再遍历第二个字符串数组，以字符串为 key 在散列表中查找，如果 value 大于零，说明存在相同字符串。时间复杂度 O(N)。 散列表 vs 二叉搜索树。散列表的插入、删除、查找操作的时间复杂度可以做到常量级的 O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是 O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？ 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。 最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间。 综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。 散列表 vs 跳表 vs 红黑树 散列表：插入删除查找都是O(1), 是最常用的，但其缺点是不能顺序遍历以及扩容缩容的性能损耗。适用于那些不需要顺序遍历，数据更新不那么频繁的。 跳表：插入删除查找都是O(logn), 并且能顺序遍历。缺点是空间复杂度O(n)。适用于不那么在意内存空间的，其顺序遍历和区间查找非常方便。 红黑树：插入删除查找都是O(logn), 中序遍历即是顺序遍历，稳定。缺点是难以实现，去查找不方便。其实跳表更佳，但红黑树已经用于很多地方了。 有一个包含 10 亿个搜索关键词的日志文件，如何快速获取到 Top 10 最热门的搜索关键词呢？可以使用的内存为 1GB. 可以选用散列表。顺序扫描这 10 亿个搜索关键词。当扫描到某个关键词时，我们去散列表中查询。如果存在，我们就将对应的次数加一；如果不存在，我们就将它插入到散列表，并记录次数为 1。以此类推，等遍历完这 10 亿个搜索关键词之后，散列表中就存储了不重复的搜索关键词以及出现的次数。然后，我们再根据前面讲的用堆求 Top K 的方法，建立一个大小为 10 的小顶堆，遍历散列表，依次取出每个搜索关键词及对应出现的次数，然后与堆顶的搜索关键词对比。如果出现次数比堆顶搜索关键词的次数多，那就删除堆顶的关键词，将这个出现次数更多的关键词加入到堆中。 假设 10 亿条搜索关键词中不重复的有 1 亿条，如果每个搜索关键词的平均长度是 50 个字节，那存储 1 亿个关键词起码需要 5GB 的内存空间，而散列表因为要避免频繁冲突，不会选择太大的装载因子，所以消耗的内存空间就更多了。而我们的机器只有 1GB 的可用内存空间，所以我们无法一次性将所有的搜索关键词加入到内存中。 可以根据哈希算法的这个特点，将 10 亿条搜索关键词先通过哈希算法分片到 10 个文件中。创建 10 个空文件 00，01，02，……，09。我们遍历这 10 亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同 10 取模，得到的结果就是这个搜索关键词应该被分到的文件编号。对这 10 亿个关键词分片之后，每个文件都只有 1 亿的关键词，去除掉重复的，可能就只有 1000 万个，每个关键词平均 50 个字节，所以总的大小就是 500MB。1GB 的内存完全可以放得下。针对每个包含 1 亿条搜索关键词的文件，利用散列表和堆，分别求出 Top 10，然后把这个 10 个 Top 10 放在一块，然后取这 100 个关键词中，出现次数最多的 10 个关键词，这就是这 10 亿数据中的 Top 10 最频繁的搜索关键词了。 合并一个逆序链表和一个正序链表// 写了两个多小时，每行代码都值得注意！！！ @_@#include &amp;lt;iostream&amp;gt;using namespace std;struct ListNode{ int val; struct ListNode* next; ListNode(int v):val(v),next(NULL) {};};void printLN(struct ListNode *p){ int i = 0; while(p) { if (i++ &amp;gt; 10) return; printf(&quot;%d-&amp;gt;&quot;, p-&amp;gt;val); p = p-&amp;gt;next; } printf(&quot;NULL\\n&quot;); return;}void rnm(struct ListNode **p1, struct ListNode **p2, struct ListNode **head, struct ListNode **tmp){ if (*p1 == NULL) return; rnm(&amp;amp;((*p1)-&amp;gt;next), p2, head, tmp); if (*head == NULL) { if (*p2 == NULL || (*p1)-&amp;gt;val &amp;lt; (*p2)-&amp;gt;val) { *head = *p1; *tmp = *p1; return; } else { *head = *p2; *tmp = *p2; *p2 = (*p2)-&amp;gt;next; } } while(*p2 != NULL &amp;amp;&amp;amp; (*p1)-&amp;gt;val &amp;gt; (*p2)-&amp;gt;val) { (*tmp)-&amp;gt;next = *p2; *tmp = *p2; *p2 = (*p2)-&amp;gt;next; } (*tmp)-&amp;gt;next = (*p1); *tmp = *p1;}struct ListNode* merge(struct ListNode *p1, struct ListNode *p2){ if (p1 == NULL) return p2; struct ListNode *head = NULL; struct ListNode *tmp = NULL; rnm(&amp;amp;p1, &amp;amp;p2, &amp;amp;head, &amp;amp;tmp); if (p2 == NULL) tmp-&amp;gt;next = NULL; else tmp-&amp;gt;next = p2; return head;}int main(){ struct ListNode a1(66), a2(44), a3(22), a4(0); struct ListNode b1(11), b2(33), b3(55), b4(77), b5(88); a1.next = &amp;amp;a2; a2.next = &amp;amp;a3; a3.next = &amp;amp;a4; b1.next = &amp;amp;b2; b2.next = &amp;amp;b3; b3.next = &amp;amp;b4; b4.next = &amp;amp;b5; struct ListNode *head = merge(&amp;amp;a2, &amp;amp;b4); printLN(head); return 0;}练习Stack[84] 柱状图中最大的矩形[42] 接雨水[739] 每日温度[496] 下一个更大元素[316] 去除重复字母[901] 股票价格跨度[402] 移掉K位数字[581] 最短无序连续子数组" }, { "title": "Kickstart Red Hat Enterprise Linux 8 by PXE server", "url": "/posts/kickstart-RHEL-8-by-PXE-server/", "categories": "学习笔记", "tags": "Linux, kickstart", "date": "2021-11-20 09:00:00 +1100", "snippet": "Kickstart RHEL 8Automated installation steps Create a Kickstart file. Prepare the Linux installation ISO or installation files. Setup PXE server. Setup Web server to provide Kickstart file and installation files. It could be replaced by NFS or FTP. Start the client machine to start the installation. (PXE needs to be supported and enabled in BIOS)Create a Kickstart file Kickstart file genertor: https://access.redhat.com/labsinfo/kickstartconfig. (Recommended by RedHat) Perform a manual installation, then get the Kickstart file at /root/anaconda-ks.cfg. Use the GUI application system-config-kickstart. Change the Kickstart file as you need. In Kickstart file, we can config everything that we can config in manual installation.Kickstart file needs to specify the installation resource via HTTP, FTP or NFS.We can set post-installation scripts in Kickstart file.Prepare PXE serverPXE server provides boot loader for client machine.DHCP server next-server to specify the IP of tftp server which holds the initial boot file. filename to specify the boot loader name on tftp server. (eg: /pxelinux.0)tftp server Put the boot image and related files on ftp server: pxelinux.0, ldlinux.c32, vmlinuz, initrd.img. These files could be found in RHEL ISO image (/isolinux/). Prepare the boot loader config file(boot menu). It’s under /pxelinux.cfg/ folder. The client machine will follow this order to find config file: mac(eg, 01-08-00-27-83-1e-2a), IP address(hex value, eg, 0a0a0ac0 means 10.10.10.192), default. We can refer to /isolinux/isolinux.cfg which is in RHEL ISO image. Specify Kickstart file in boot loader config file by using ks=. eg, append initrd=initrd.img ks=http://192.168.8.8/ks/rhel.cfgWeb serverDuring installation, the client machine needs to download installation files from http server. eg, http://192.168.8.8/rhel" }, { "title": "Linux网络-接收数据包", "url": "/posts/Linux-network-rcv/", "categories": "学习笔记", "tags": "Linux, network", "date": "2021-10-15 13:00:00 +1100", "snippet": " 基于Linux kernel 6.0Linux内核收包总览sequenceDiagram participant Network participant NIC participant CPU participant Kernel participant User Network-&amp;gt;&amp;gt;NIC: 数据帧从外部网络到达网卡 NIC-&amp;gt;&amp;gt;Kernel: 网卡把帧 DMA 到内存RingBuffer NIC-&amp;gt;&amp;gt;CPU: 网卡硬中断通知 CPU CPU-&amp;gt;&amp;gt;Kernel: CPU 响应硬中断，简单处理后发出软中断 Kernel--&amp;gt;&amp;gt;Kernel: ksoftirqd线程处理软中断，调用网卡驱动注册的poll函数开始收包 Note right of Kernel: 帧被从RingBuffer上取下来保存为一个skb Kernel-&amp;gt;&amp;gt;User: 协议层开始处理帧，处理完后被放到socket的接收队列中Linux启动 kernel/softirq.c 创建 ksoftirqd并不直接以函数调用的方式创建，通过 early_initcall(spawn_ksoftirqd)，将spawn_ksoftirqd加入vmlinux文件的.init段，在程序启动时，即被调用。 include/linux/init.h 中的 early_initcall(fn) init.h中也有其他的一些 *_initcall(fn)，early_initcall(fn)在初始化SMP前就被运行，只用于build-in的代码，不能用于modules。spawn_ksoftirqd 函数被加入vmlinux的.init段后，最终会被do_initcalls()调用。start_kernel # init/main.c -&amp;gt; rest_init() # init/main.c -&amp;gt; kernel_thread(kernel_init, NULL, CLONE_FS | CLONE_FILES); # init/main.c -&amp;gt; kernel_init() -&amp;gt; kernel_init_freeable(); -&amp;gt; do_basic_setup(); -&amp;gt; do_initcalls();" }, { "title": "在 WSL 和 CentOS 7 中编译openjdk-17-ga", "url": "/posts/build-openjdk-17-ga/", "categories": "学习笔记", "tags": "java", "date": "2021-09-16 13:00:00 +1000", "snippet": "昨天 openjdk 发布了 17GA，折腾了会儿在 Windows(WSL) 和 CentOS 7.9 上都编译出来了。记录一下。Windows(WSL)Windows下可以用Cygwin或者WSL编译openjdk，因为我已经装了WSL了，所以就用WSL来编译。下载完 openjdk-17-ga 的zip文件，解压，运行 wsl，并进入解压后的目录（比如我的是/mnt/d/GitHub_Repo/jdk-jdk-17-ga）。使用下面的步骤安装： 首先需要安装编译所需要的一些依赖文件，有些是我以前装的包，不确定是不是 openjdk 的编译也需要，我都包括进来了。sudo apt install gcc autoconf make zip unzip build-essential libx11-dev libxext-dev libxrender-dev libxtst-dev libxt-dev libcups2-dev libfreetype6-dev libasound2-dev ccache gawk m4 libasound2-dev libxrender-dev xorg-dev xutils-dev binutils libmotif-dev ant mercurial libc6-dev 安装 boot JDK。需要装一个比当前版本低的jdk，我这里装的是openjdk-16。sudo apt install openjdk-16-jdk 运行 configurechmod u+x configure./configure --build=x86_64-unknown-linux-gnu --host=x86_64-unknown-linux-gnu这一步如果出错，可以查看详细的出错信息。成功后会出现：====================================================A new configuration has been successfully created in/mnt/d/GitHub_Repo/jdk-jdk-17-ga/build/linux-x86_64-server-releaseusing configure arguments &#39;--build=x86_64-unknown-linux-gnu --host=x86_64-unknown-linux-gnu&#39;.Configuration summary:* Name: linux-x86_64-server-release* Debug level: release* HS debug level: product* JVM variants: server* JVM features: server: &#39;cds compiler1 compiler2 epsilongc g1gc jfr jni-check jvmci jvmti management nmt parallelgc serialgc services shenandoahgc vm-structs zgc&#39;* OpenJDK target: OS: linux, CPU architecture: x86, address length: 64* Version string: 17-internal+0-adhoc.slupro.jdk-jdk-17-ga (17-internal)Tools summary:* Boot JDK: openjdk version &quot;16.0.1&quot; 2021-04-20 OpenJDK Runtime Environment (build 16.0.1+9-Ubuntu-120.04) OpenJDK 64-Bit Server VM (build 16.0.1+9-Ubuntu-120.04, mixed mode, sharing) (at /usr/lib/jvm/java-16-openjdk-amd64)* Toolchain: gcc (GNU Compiler Collection)* C Compiler: Version 9.3.0 (at /usr/bin/gcc)* C++ Compiler: Version 9.3.0 (at /usr/bin/g++)Build performance summary:* Cores to use: 12* Memory limit: 51360 MBWARNING: Your build output directory is not on a local disk.This will severely degrade build performance!It is recommended that you create an output directory on a local disk,and run the configure script again from that directory. 运行makemake images成功后会出现Finished building target &#39;images&#39; in configuration &#39;linux-x86_64-server-release&#39;。 检查是否编译成功，可以看到openjdk的版本是17-ga了。slupro@DESKTOP-96TK3JA:/mnt/d/GitHub_Repo/jdk-jdk-17-ga$ ./build/linux-x86_64-server-release/images/jdk/bin/java -versionopenjdk version &quot;17-internal&quot; 2021-09-14OpenJDK Runtime Environment (build 17-internal+0-adhoc.slupro.jdk-jdk-17-ga)OpenJDK 64-Bit Server VM (build 17-internal+0-adhoc.slupro.jdk-jdk-17-ga, mixed mode, sharing)CentOS 7.9CentOS在安装时由于组件和包都可选，每个人装的可能不一样，只列我的安装步骤了： 安装常用的开发组件和一些依赖文件sudo yum groupinstall &quot;Development Tools&quot;sudo yum install libXtst-devel libXt-devel libXrender-devel libXrandr-devel libXi-devel freetype-devel cups-devel alsa-lib-devel libffi-devel fontconfig-devel 安装boot JDKsudo yum install java-16-openjdk-devel 运行 configurechmod u+x configure./configure --build=x86_64-unknown-linux-gnu --host=x86_64-unknown-linux-gnu这一步如果出错，可以查看详细的出错信息。成功后会出现：====================================================A new configuration has been successfully created in/home/slupro/soft/jdk-jdk-17-ga/build/linux-x86_64-server-releaseusing default settings.Configuration summary:* Name: linux-x86_64-server-release* Debug level: release* HS debug level: product* JVM variants: server* JVM features: server: &#39;cds compiler1 compiler2 epsilongc g1gc jfr jni-check jvmci jvmti management nmt parallelgc serialgc services shenandoahgc vm-structs zgc&#39; * OpenJDK target: OS: linux, CPU architecture: x86, address length: 64* Version string: 17-internal+0-adhoc.slupro.jdk-jdk-17-ga (17-internal)Tools summary:* Boot JDK: openjdk version &quot;16.0.1&quot; 2021-04-20 OpenJDK Runtime Environment 21.3 (build 16.0.1+9) OpenJDK 64-Bit Server VM 21.3 (build 16.0.1+9, mixed mode, sharing) (at /usr/lib/jvm/java-16-openjdk-16.0.1.0.9-3.rolling.el7.x86_64)* Toolchain: gcc (GNU Compiler Collection)* C Compiler: Version 4.8.5 (at /usr/bin/gcc)* C++ Compiler: Version 4.8.5 (at /usr/bin/g++)Build performance summary:* Cores to use: 4* Memory limit: 15866 MB 运行makemake images我在这里遇到了一个错误：=== Output from failing command(s) repeated here ===* For target hotspot_variant-server_libjvm_objs_precompiled_precompiled.hpp.gch:gcc: error: unrecognized command line option &#39;-std=c++14&#39;写了一个简单的C++文件，用 gcc 带参数 -std=c++14 运行，发现不支持这个参数。搜索了一下发现自带的gcc版本是4.8.5，不支持这个写法，于是把gcc升级到新的版本7.3.1。sudo yum install centos-release-sclsudo yum install devtoolset-7-gcc*scl enable devtoolset-7 bashgcc --version注意升级完gcc后，需要重新运行./configure配置，然后再make images即可成功。" }, { "title": "Kubernetes 学习笔记", "url": "/posts/learn-kubernetes-in-one-page/", "categories": "学习笔记", "tags": "kubernetes, cloud", "date": "2021-09-13 12:00:00 +1000", "snippet": "介绍学习《kubernetes in action》时做的笔记，书中使用的k8s的版本和目前的版本有差距，但大体概念没有变。转载请注明来自：https://slupro.github.io/k8s的需求 从单一应用到微服务：满足可横向和纵向扩展的需求，聚焦于总的资源池。 为开发和部署提供一致的环境：避免了环境差异的问题，比如服务间库的冲突等。 持续交付：DevOps和NoOps：一个团队参与开发、部署、运维，是为DevOps，Dev更多地接触生产中的应用，能帮助理解用户需求和问题，更好理解运维团队维护应用所面临的困难。开发者直接部署，不需要系统管理员的帮助，NoOps，系统管理员只关注底层基础设置运转正常。Docker Linux namespace: 限制view：文件、进程、网络接口、主机名等。 Linux cgroups：限制进程能用的资源量：CPU、mem、带宽等。Pod特点 pod 是逻辑主机，k8s 管理的最小单位 pod 上可以运行一个或多个容器，建议一个 同一 pod 下的容器使用相同的 network 和 UTS 命名空间，这些容器共享相同的IP和端口空间 尽量将容器分散到不同的pod中，除非这些容器间紧耦合，横向扩容时也需要同时扩容 一个 pod 不可以跨节点部署命令查看pods信息kubectl get podskubectl get po pod_name -o yaml使用yaml来创建podkubectl create -f aaaa.yaml查看pod中容器的日志kubectl logs pod_name// 若pod中有多个容器，使用-c指定容器kubectl logs pod_name -c docker_container_name将本地端口映射到pod中的端口// 映射本地8888到远程pod的8080端口kubectl port-forward pod_name 8888:8080解释pods字段的含义kubectl explain podskubectl explain pod.spec使用 labels 来组织 pod可以给一个pod添加多个标签，label 以 key=value 的方式配置。比如 release=beta 或者 release=prod。// 列出 pods 时同时显示标签kubectl get po --show-labels// 列出指定的标签内容kubectl get po -L env,release// 添加或者修改现有标签，修改需 --overwritekubectl label po pod_name env=debug --overwrite可通过 label 来过滤 pod：包含特定key的标签，包含特定key、value的标签，取非。可以有：env!=prod, env in (prod,debug), env not in (prod,debug)。// 列出包含env的所有podkubectl get po -l env// 列出env=prod且app=service的所有podkubectl get po -l env=prod,app=service// 列出不包含env的所有pod，需要用单引号来避免感叹号被bash shell解释kubectl get po -l &#39;!env&#39; 事实上，label 可以附加到任何 k8s 对象上，包括节点，方便调度。用 namespace 来分隔资源labels 分隔，有可能相互重叠，比如一个对象属于A label，又属于B label。于是有了 namespace分隔，对象不会重叠。// 列出集群中的所有nskubectl get ns// 列出指定ns的podkubectl get po -n xxx namespace并不从网络上隔离资源。删除 pod// 指定名称kubectl delete po a-name// 使用标签kubectl delete po -l env=debug// 使用namespacekubectl delete ns xxx// 删除当前 ns 的所有podkubectl delete po --all// 删除当前 ns 的（几乎）所有资源，包括pod和servicekubectl delete all --all谁管理 pod检查pod是否正常工作 livenessProbelivenessProbe三种检测方式： HTTP GET，检查状态码 TCP是否能建立连接 Exec在容器内运行命令，并检查退出状态// 容器崩溃重启，查看前一个容器的日志kubectl logs mypod --previous// 查看容器重启的原因kubectl describe po mypod检查pod是否就绪 readinessProbe如果 pod 中的服务启动较慢，不能马上对外提供服务，则需要用 readinessProbe 来检测 pod 已正常工作，再使用该 pod。和 livenessProbe 类似，readinessProbe 也有三种检测方式。区别是： livenessProbe 如果检测失败，k8s会终止或重启pod。 readinessProbe 如果检测失败，不会终止或重启pod。service不会将请求转发给该pod。# 用 ls 检查 /var/ready 是否存在，存在返回退出码0。kind: ReplicationControllerspec: template: spec: containers: - name: test-container image: docker_image readinessProbe: exec: command: - ls - /var/readyReplication Controller (会被ReplicaSet取代!)Replication Controller 持续监控来确保运行了指定的 pod，及其数量。主要有3部分： label selector replica count pod template 更改 label selector 和 pod template 会让 RC 已经运行的pod脱离RC的控制，RC 会关注新创建的pod。老的pod继续运行，并不受RC的监控。删除一个RC时，其管理的pod会被先行删除。可以用 – cascade=false 来保持pod不被关闭。// 显示 RC 的总体情况kubectl get rc// 显示 RC 的详情kubectl describe rc xxxx// 修改RC模板kubectl edit rc xxxx// 水平扩容kubectl scale rc xxxx --replicas=10// 删除 RCkubectl delete rc xxxx --cascade=falseReplicaSetReplicaSet 的行为与 ReplicationController 完全相同，但 pod selector 的能力更强。配置：apiVersion: apps/v1kind: ReplicaSetmetadata: name: kubiaspec: replicas: 3 selector: matchLabels: app: kubia template: metadata: labels: app: kubia spec: containers: - name: kubia image: luksa/kubia// 创建kubectl create -f aaa.yamlkubectl get rskubectl describe rskubectl delete rs kubiaDaemonSet前面的 RC 和 RS 是在集群中运行特定数量的 pod，如果需要在每个节点上运行，比如日志收集 或 资源监控，则需要用 DaemonSet。nodeSelector 可以让 DaemonSet 部署 pod 到指定的节点。 DaemonSet 目的是运行系统级的服务，所以即使在不可调度的节点，DaemonSet 也可部署 pod。// DaemonSet yamlkind: DaemonSetspec: template: spec: nodeSelector: disk: ssd// 查询 DSkubectl get ds// 查询节点信息kubectl get node// 节点添加disk=ssd标签，需要修改时用参数 --overwritekubectl label node aaaaa disk=ssdJobReplication Controller, ReplicaSet, DaemonSet 都是保证 pod 一直在运行，如果失败，会重新运行一个 pod。如果需要 pod 只执行一次，那么就可以用 Job。kind: Jobspec: template: spec: restartPolicy: OnFailure// 查看 jobkubectl get jobs// 使用 -a 可查看已经退出的jobkubectl get pod -aJob 可以创建多个 pod 实例，以串行completions或者并行parallelism的方式运行。// 顺序运行 5 次spec: completions: 5 // 5 个 pod 执行完，最多 2 个可以同时运行spec: completions: 5 parallelism: 2 可以设置 activeDeadlineSeconds 来限制 pod 运行的时间，超过此时间了将终止 pod，并标记为失败。spec.backoffLimit，可以配置 Job 在失败前被重试的次数，默认为6。CronJobCronJob 类似于 Linux 的 cron，定时执行任务。分钟，小时，每月的第几天，月，星期。 可以通过 startingDeadlineSeconds 来指定最晚不能超过的时间，如果超过，则显示为 Failed。kind: CronJobspec: schedule: &quot;0,30 * * * *&quot; startingDeadlineSeconds: 15Service由于 Pod 随时可能会创建或者销毁，所以要和 Pod 内容器通信需要提供方式能让客户端找到 Pod 的IP和端口，service用来解决这个问题。Service 为一组功能相同的 pod 提供单一不变的接入点。客户端通过IP、端口号和service建立连接，这些连接会被路由到提供该服务的任意一个 pod 上。Service 的创建可以用yaml或者 kubectl expose 命令创建service。 如果需要将同一客户端的请求都重定向到后台同一个 pod，需要使用 spec.sessionAffinity: ClientIP，默认该属性为None。 // 所有app=labelname 的pod都属于该service。kind: Servicespec: sessionAffinity: ClientIP ports: - port: 80 targetPort: 8080 selector: app: labelname 服务发现两种方式： 通过环境变量。service 用的 IP地址和端口，会在 pod 的环境变量中保存，podname_SERVICE_HOST 和 podname_SERVICE_PORT。 通过DNS。kube-system 命名空间下有个 kube-dns pod，上面运行了DNS，集群中的其它pod都被配置成使用kube-dns当作DNS server（修改了/etc/resolv.conf）。pod是否使用内部的DNS通过 spec.dnsPolicy 来确定。// 查询 service，可看到集群内通信用的 cluster-ipkubectl get svc// 查询环境变量kubectl exec xxx env FQDN 命名规则： backend-service.default.svc.cluster.local其中，backend-service对应service名称，default对应namespace，svc.cluster.local是可配置的集群域后缀。如果在一个命名空间下，可以直接用 backend-service 来访问。集群内的 pod 连接集群外的服务（通过IP） 可以利用service的负载均衡和服务发现，让集群中的客户端pod可以像连接内部服务一样连接外部服务。service 并不直接和 pod 直接相连，Endpoint 资源介于 service 和 pod 之间，Endpoint 资源就是一组IP地址和端口列表。可以单独的创建没有pod的 service ，然后为其创建 Endpoint 资源。// 查看service的细节kubectl describe svc xxxx// 查看endpointskubectl get endpoints创建serviceapiVersion: v1kind: Servicemetadata: name: slutest-servicespec: ports: - port: 80为该service创建endpoint，两者的 metadata.name 必须一样才能关联。apiVersion: v1kind: Endpointsmetadata: name: slutest-servicesubsets: - addresses: - ip: 12.12.12.12 - ip: 22.22.22.33 ports: - port: 80集群内的 pod 连接集群外的服务（通过域名）需要指定 service 的spec.type 为 ExternalName。集群内的pod可以通过域名 ext-svc 访问外部的 api.externalsite.com。kind: Servicemetadata: name: ext-svcspec: type: ExternalName externalName: api.externalsite.com集群内的 pod 对外提供服务 (NodePort)创建一个 NodePort 类型的 service，可以在所有节点上都开放一个相同端口号的端口，任何一个节点接收到传入的连接，都会转发给提供服务的 Pod。下面的例子中将开放每个集群节点的1234端口，可以访问每个节点的1234端口，或者cluster-ip的80端口，数据包会被转发到service对应pod的8080端口。kubectl get svc 可以查看 CLUSTER-IP。 NodePort 需要打开节点上 1234 端口的防火墙。Load Balancer 方式则不需要修改防火墙配置。但是，如果只通过一个节点的IP访问集群服务，该节点挂掉时，就无法访问了。所以有了后面的负载均衡方式。kind: Servicespec: type: NodePort ports: - port: 80 targetPort: 8080 nodePort: 1234 selector: app: outservice集群内的 pod 对外提供服务 (Load Balancer)Load Balancer 拥有自己唯一的可公开访问的IP地址，并将所有连接重定向到服务。LB是NodePort的一个扩展，如果k8s在不支持LB的环境中运行，则该服务仍以NodePort方式运行。当启用 LB 时，IP地址将在 kubectl get svc 中的 EXTERNAL-IP 存在。 NodePort 需要打开节点上 1234 端口的防火墙。Load Balancer 方式则不需要修改防火墙配置。kind: Servicespec: type: LoadBalancer spec.externalTrafficPolicy: Local该配置可以让在 NodePort 环境下，接收到请求的节点只将请求转发到本节点内的 pod。优点是减少了转发到其它节点时的网络hop。缺点是如果本地没有对应的pod，则连接会挂起，另外如果在LB环境中，由于LB是根据节点做转发，每个节点的pod数不一定相同，可能造成各个 pod 的压力不同。当使用 NodePort 模式时，由于可能在节点间转发(SNAT)，所以后端pod可能无法获取到正确的请求的源IP地址，使用 spec.externalTrafficPolicy: Local 时，由于无跳转，可以获取到正确的源IP地址。集群内的 pod 对外提供服务（Ingress）Ingress 功能的支持因不同的 Ingress controller 而定。目前只工作在应用层，支持http/https负载均衡可以根据 url 进行转发，并可提供 session affinity 等功能。 Ingress controller 会将请求直接发送给一个选择的 pod，不会再经过 service 了。// 使用下面命令查询 ingress 的IP地址，然后修改客户端DNS，将域名指向该IP地址。kubectl get ingresses Ingress 处理HTTPS请求：需要创建 Secret 资源，将证书和私钥保存到 Secret 上，并将 Secret 附加到 Ingress 上。请求在客户端和Ingress之间是HTTPS连接，在Ingress和pod之间是HTTP。kubectl create secret tls my-secret --cert=cert_file --key=key_file# Ingress 将www.test.com映射到后面的service-nodeport，80端口# rules 和 paths 可以设置多个值，以便细分处理不同域名，不同url的请求。# 如果不用 TLS，则不需要 TLS 部分kind: Ingressspec: tls: - hosts: - www.test.com secretName: my-secret rules: - host: www.test.com http: paths: - path: /foo1 backend: serviceName: service-nodeport1 servicePort: 80获取 service 内所有 pod 的 IP（headless）设置 Service 的 spec.clusterIP: None，则可以创建一个 headless 的服务，该服务不会获得cluster IP，而是会返回该service下所有pod的IP。kind: Servicespec: clusterIP: NoneVolume 卷K8s 中的卷是 pod 的一个组成部分，因此和容器一样定义在 pod yaml 中。一个pod中的所有容器都可以使用相同的卷，但必须挂载。卷的生命周期和 pod 的生命周期一样，在pod创建时才会存在。Volume的类型 emptyDir: 存储临时数据的空目录 hostPath: 将目录从节点的文件系统挂载到 pod 中 gitRepo: checkout git repo的内容来初始化卷 gcePersistentDisk, awsElasticBlockStore, azureDisk: 用于cloud环境 configMap, secret, downwardAPI: 将k8s部分资源和集群信息mount到podemptyDir当medium为memory时，k8s将在tmfs文件系统（内存）上创建emptyDir。# 两个容器，共享同一个卷，各自加载到自己的不同目录了。kind: Podspec: containers: - image: ubuntu name: u1 volumeMounts: - name: mem mountPath: /tmp1 - image: ubuntu name: u2 volumeMounts: - name: mem mountPath: /tmp2 readOnly: true volumes: - name: mem emptyDir: medium: Memory详细示例参考：https://github.com/slupro/kubernetes-config-examples/blob/main/examples/ContainersInOnePod.yaml# 进入容器 u2kubectl exec -it uuu -c u2 -- /bin/bashgitRepogitRepo 先创建pod，k8s 再创建一个空目录(emptyDir)，然后clone指定的git仓库到本地，最后再挂载目录，启动容器。 gitRepo 里的内容并不会主动和git仓库同步，只有创建时会同步。可以用于从git上下载网站静态HTML文件，并创建一个包含nginx的容器。# gitRepo.directory 指定将repo克隆到卷的位置spec: volumes: - name: volume1 gitRepo: repository: https://github.com/aaaa.git revision: master directory: .hostPath 可让 pod 内的容器访问 node 上的文件或目录。可让同一节点上的多个 pod 访问相同的文件或目录。 持久型存储，pod删除时不会删除hostPath。 尽量避免使用 hostPath 持久化pod的数据，因为和节点绑定了。除非在需要访问节点数据时，再使用。其它持久化存储不同的Cloud基础设施提供了不同的方式，比如 Google Cloud 的 GCE persistent disk，AWS 的 awsElasticBlockStore，Azure的azureFile或者AzureDisk。kind: Podspec: volume: - name: dbdata awsElasticBlockStore: volumeId: myvol fsType: ext4# 使用 NFS 卷spec: volume: - name: dbdata nfs: server: 22.3.3.3 path: /a/pathPersistentVolume and PersistentVolumeClaim开发人员不需要了解底层实际使用的存储技术，只需要创建pod，指明需要使用的PVC；PVC中包含存储大小和访问模式的需求；运营人员创建PV，来满足PVC的需要。创建PV和PVC 创建持久卷# 容量5G，可以支持单节点和多节点的读写，删除时保留kind: PersistentVolumemetadata: name: mysql-pvspec: capacity: storage: 5Gi accessModes: - ReadWriteOnce - ReadOnlyMany persistentVolumeReclaimPolicy: Retain hostPath: path: /temp/k8s 持久卷 PV 不属于 任何namespace，它和节点一样是集群层面的资源。持久卷声明 PVC 属于 namespace。PV 和 PVC 状态是Bound时，是绑定的。// 查看持久卷kubectl get pv 创建持久卷声明PVC来获取持久卷PVPVC 和 pod 是相互独立的，这样如果pod被删除并重新创建后，扔可以使用之前的 PVC。 如果要使用手动生成的PV，需要在PVC里设置 spec.storageClassName 为 ““，空字符串。否则PVC会使用默认的storageClassName生成动态PV。# 申请1G的存储空间，允许单个客户端访问（读写）kind: PersistentVolumeClaimmetadata: name: mysql-pvcspec: resources: requests: storage: 1Gi accessModes: - ReadWriteOnce storageClassName: &quot;&quot; ACCESS MODES 说明：RWO - ReadWriteOnce，允许单个节点读写ROX - ReadOnlyMany， 允许多个节点，只读RWX - ReadWriteMany，允许多个节点读写// 查看 pvc 是否已经绑定 pvkubectl get pvc 在 pod 中使用 PVCkind: Podspec: containers: volumes: - name: mysql-data persistentVolumeClaim: claimName: mysql-pvc回收 PVPV 通过配置 persistentVolumeReclaimPolicy 来设置PV的回收方式： Retain：PVC被删除后，PV被保留，若要重新使用该PV，需要手动删除并重新创建，手动删除PV前可以处理其中的文件。PVC被删除后，PV状态变为 Released，新创建PVC状态会是Pending，因为老的PV已不可用。 Recycle：和 Retain 的区别是，Recycle可以被不同的PVC再次申明和使用。 Delete：PVC被删除时，PV也被删除。可以修改现有PV的回收策略。持久卷的动态生成管理员可以创建一个或多个 StorageClass 对象，用户在 PVC 中引用该 StorageClass，则可自动创建 PV。StorageClass需要使用对应的云提供商的provisoner。apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: ssddiskprovisioner: k8s.io/minikube-hostpathparameters: type: pd-ssd k8s中会有一个默认的sc，如果pvc没有指明使用哪个StorageClass，则会使用默认SC。// 查看StorageClasskubectl get sc// 查看默认的standard sc的定义kubectl get sc standard -o yamlPVC 从 StorageClass 请求资源kind: PersistentVolumeClaimmetadata: name: test-pvcspec: storageClassName: ssddisk resources: requests: storage: 100Mi accessModes: - ReadWriteOnce配置应用程序：ConfigMap 和 Secret向容器传递命令行参数Dockerfile 中的 ENTRYPOINT 和 CMD ENTRYPOINT 定义容器启动时，调用的可执行程序。 CMD 指定传递给 ENTRYPOINT 的参数。两种运行方式： shell (ENTRYPOINT node app.js)：用shell调用node app.js，相当于 /bin/sh -c node app.js，主进程（pid1）是shell进程。 exec (ENTRYPOINT [“node”, “app.js”])：容器启动时直接调用 node app.js，主进程是node。FROM ubuntu:latestRUN apt-get update; apt-get -y install some_appADD file1 /bin/file1ENTRYPOINT [&quot;/bin/file1&quot;]CMD [&quot;arg1&quot;, &quot;arg2&quot;]启动容器时修改参数docker 中可以在启动的时候重新覆盖命令行参数：docker run some_image arg3k8s 可以在配置中，使用 command(=ENTRYPOINT) 和 args(=CMD) 覆盖。容器启动后，不能修改启动参数了。kind: Podspec: containers: image: some_image command: [/bin/file1] args: [&quot;arg4&quot;, &quot;arg5&quot;]args也可以用下面的方式：# 字符串可以不要双引号，数字需要双引号。spec: containers: args: - arg6 - arg7 - &quot;33&quot;为容器配置环境变量kind: Podspec: containers: image: some_image env: - name: some_key value: some_value这样只能 hard code 一个值，如果想解耦，可以使用 valueFrom 来从 ConfigMap 中读取数据。ConfigMap 解耦配置ConfigMap介绍 k8s 的 ConfigMap 使用 etcd 保存配置，所以是保存key/value映射。 应用程序不用直接读取 ConfigMap，key/value 的信息以环境变量或者卷文件的方式传递给容器，从而更灵活。 应用程序也可以通过k8s Rest API读取ConfigMap中的内容。 为不同的环境创建不同的ConfigMap（dev,production），pod相同。ConfigMap的创建 通过命令行 –from-literal 创建。kubectl create configmap mycm --from-literal=key1=value1 --from-literal=key2=value2 通过 yaml 文件创建data: key1: value1 key2: value2kind: ConfigMap 通过文件或目录创建，可以将文件的内容加载到key。没有指定key时，使用文件名作为key。kubectl create configmap mycm --from-file=xxx.conf --from-file=key=somefile --from-file=folder/# 查看cm内容kubectl describe cm mycmkubectl get cm mycm -o yaml将ConfigMap的内容传递给容器的环境变量个别变量可以通过 valueFrom 来引用，或者用 envFrom 来引用所有条目作为环境变量。如果创建pod时，引用的ConfigMap不存在，受影响的容器会启动失败，其它pod中的容器可以正常启动。当CM中的信息补全后，失败的容器会自动启动。也可以设置成CM信息不存在也继续启动容器，设置 configMapKeyRef.optional=true。 当有非法的环境变量key时，该环境变量会被忽略，忽略时不会发出事件通知。比如key中包括-。# 容器xx中将存在环境变量 env-key，值从ConfigMap mycm的key_in_cm中获取。# 容器xxx中将会有mycm中的所有key:value在环境变量中，key会被添加前缀CONFIG_。prefix是可选的。kind: Podspec: containers: - image: xx env: - name: env_key valueFrom: configMapKeyRef: name: mycm key: key_in_cm - image: xxx envFrom: - prefix: CONFIG_ configMapKeyRef: name: mycmconfigMap卷 可以把配置加载成volumeconfigMap卷 默认会把CM中的每个条目均暴露成一个文件。key为文件名，value为内容。如果需要选择暴露的key，需要使用spec.volumes.configMap.items.key来指定。kind:Podspec: containers: - image: xx volumeMounts: - name: vconfig mountPath: /etc/conf.d readOnly: true volumes: - name: vconfig configMap: name: mycm items: - key: key_in_cm path: my.conf configMap卷加载后，容器镜像中原有目录将被清空。如果不需要清空原有目录，需要使用 spec.containers.volumeMounts.subPath 来指定文件名。configMap卷中文件的默认权限被设置为644(-rw-r-r–)，可以通过 volumes.configMap.defaultMode 来修改文件的权限。环境变量和启动参数无法在不重启容器的前提下修改，但configMap卷可以不重启容器就修改。用Secret传递敏感数据Secret简介 Secret 与 ConfigMap 类似，也是key/value映射。 Secret 与 ConfigMap 类似，条目可以通过环境变量传递给容器，可以暴露为volume中的文件，只分发到需要访问的pod上。 Secret 之会保存在节点的内存中，不会写入物理存储，这样从节点上删除secret就不用擦除磁盘了。 k8s 1.7开始，etcd会加密保存Secret。默认Secret token系统会有一个默认的Secret，并默认会挂载到每个容器中。// 查看kubectl describe secretskubectl get secrets// 查看pod中default secret挂载的位置kubectl describe pod创建 Secret# 将afolder目录下的所有文件以及aaa文件映射到mysec Secret上。文件名为key，内容为value。kubectl create secret generic mysec --from-file=afolder --from-file=aaa挂载 Secret 到 Podkind: Podspec: containers: image: xxx volumeMounts: - name: vol_name mountPaht: /etc/cert/ readOnly: true volumes: - name: vol_name secret: secretName: mysec 也可以通过环境变量暴露：env.valueFrom.secretKeyRef。Secret也可以保存证书，k8s用来从私有仓库里面获得镜像。需要做两件事：创建包含Docker镜像仓库证书的Secret，在Pod的yaml定义中指定需要使用的Secret(spec.imagePullSecrets)。Secret 和 ConfigMap 的区别当以 yaml 格式查看 Secret 时，可以看到其中的value都被 Base64 编码了，所以Secret也可以保存二进制的文件作为value。Secret的value大小限制为1MB。但是 ConfigMap 不能保存二进制的信息。kubectl get secret mysec -o yamlDeployment更新pod两种方式更新：先删pod v1，再创建pod v2。先创建pod v2，再删pod v1。第一种方式会导致服务暂时不可用，第二种方式会需要更多的硬件资源。先删v1，再创建v2如果用ReplicationController来管理pod的话，直接更新其中的pod模板。先创建v2，再删v1蓝绿部署(blue-green deployment)：使用service的标签选择器将流量切换到新pod，正常后再删除旧版本的pod。使用kubectl set selector。用 ReplicationController 实现自动滚动升级kind: ReplicationControllermetadata: name: testrc-v1使用 testrc-v2 滚动替换 v1: kubectl rolling-update testrc-v1 testrc-v2 --image=xxx:v2 --v 6。通过参数 --v 6 来显示替换的详细日志。 已不再使用 rolling-update，因为 kubectl 只是执行滚动升级的客户端，具体执行操作是一步步通过rest发送到K8S API完成，如果升级中网络出错，升级将会中断。DeploymentDeployment是更高层的资源使用方式，ReplicationController和ReplicaSet更底层。在使用 Deployment 时，pod是由Deployment底层的ReplicaSet创建和管理的。创建 DeploymentDeployment创建的定义方式和ReplicationController类似，只是 kind 为 Deployment。# 创建Deployment，--record 会记录历史版本号kubectl create -f xxx.yaml --record# 查看kubectl get deploymentkubectl describe deployment# Deployment 是使用 ReplicaSet 管理podkubectl get replicasets使用 Deployment 升级Deployment升级有两种方式： RollingUpdate：默认升级策略，会创建新的pod，并逐渐删除旧的pod。底层的ReplicaSet不会被删除，恢复时使用。 Recreate：一次性删除所有旧pod，再创建新pod，中间会有不可用的时候。# 修改单个或少量资源属性时，可使用 kubectl patch。# minReadySeconds 设置滚动升级的速度。kubectl patch deployment xxx -p &#39;{&quot;spec&quot;: {&quot;minReadySeconds&quot;: 10}}&#39;# 将deployment_name的pod模板中的镜像改为image_name:v2。触发滚动升级。kubectl set image deployment xxx container_name=image_name:v2# status可以查看升级的详细过程kubectl rollout status deployment xxx# 使用pause或resume暂停或恢复滚动升级，用于Canary金丝雀发布kubectl rollout pause/resume deployment xxx 控制滚动升级速率：maxSurge 和 maxUnavailable 属性。minReadySeconds，新pod至少运行多久后才视为可用。默认情况下，超过10min不能完成滚动升级，将被认为失败。可以通过设置progressDeadlineSeconds来设定。回滚 Deployment 的升级 升级后不应手动删除 ReplicaSet 的信息，这些信息保存着历史版本，以便进行回滚。手动回滚：# 回滚到上一个版本。undo命令可以在滚动升级的过程中执行。kubectl rollout undo deployment xxx# 查看升级的历史版本kubectl rollout history deployment xxx# 回滚到指定版本kubectl rollout undo deployment xxx --to-revision=1StatefulSetReplicaSet 和 ReplicationController 创建的新pod和被替换的pod拥有不同的名称、网络标识和状态。为了保证这些信息在新的pod中保持一致，引入了 StatefulSet。使用 StatefulSet StatefulSet 缩容的时候一次只操作一个pod，如果有实例不健康，StatefulSet也不会缩容。避免集群中多个节点出问题时丢数据。由于一个持久卷声明PVC被删除时，对应的持久卷PV也会被删除。但在 StatefulSet 中，即使缩容，也不删PVC，否则删除PV会造成数据丢失。扩容时，StatefulSet新增的pod会使用旧的PVC，也就是会使用之前pod使用的PVC，从而访问相同的数据。 at most one: k8s 保证不会有两个相同标记和绑定相同PVC的pod同时运行。 创建StatefulSet的service时，spec.clusterIP 必须是 None，也就是headless service。 ReplicaSet 会一次创建所有的pod，但 StatefulSet 会在前一个pod Running后再创建下一个pod。这对于需要选举的节点来说，StatefulSet更安全。 缩容的时候，会先删除拥有最高索引的pod。DNS 中的 SRV 记录可以在DNS中查询 SRV 记录来获得StatefulSet中的其它pod地址。SRV记录返回的顺序是随机的。# 列出 SRV 记录dig SRV pod.default.svc.cluster.local修改 StatefulSetkubectl edit statefulset xxx修改StatefulSet时： 如果修改了image的地址，k8s并不会更新已经有的pod。 如果spec.replicas增加了，会创建新的pod。节点失效当一个节点失效（网络掉了、断电等）后，该节点上的 kubelet 无法与 k8s API 通信，所以k8s认为该节点 NotReady(kubectl get node)，节点上的pod的状态为 Unknown。该pod的状态在超时(可配)未更新后，k8s将该pod标记为Terminating(虽然该pod还在断线的节点上运行)。由于k8s已无法通知离线的节点来删除该pod，所以可以手动强制删除kubectl delete po xxx --force --grace-period 0。k8s 架构API server对外暴露了ComponentStatus接口，可以使用 kubectl get componentstatuses 查询。 k8s各个组件间只能通过API server进行通信，组件间不会直接通信。包括访问etcd也需要通过api server。Control panel上的组件可以分散到多台服务器上，每个组件也可以运行多个实例保证高可用。其中etcd和api server可以多实例同时运行，scheduler和controller manager只能有一个实例在运行，其它实例待命。Control panel上的组件以及kube-proxy可以直接部署在系统上，也可以作为pod来运行。kubelet是唯一一个以系统组件来运行的，由kubelet来运行pod。所以要把control panel作为pod来运行的话，需要把kubelet部署在master上。etcd k8s 使用etcd来持久化信息，包括pod、replicaSet、service等等。只有api server可以和etcd通信。 api server 使用乐观锁(OptimisticLock)控制对etcd数据的更新，每个对象上含有一个metadata.resourceVersion字段。 多个etcd使用 RAFT 一致性算法来达成一致。 etcd实例应该是奇数。因为只有超过总数半数的实例在线才能达成共识，所以2个实例比1个实例更糟，挂掉的概率增加了一倍。比如4个节点，需要3个节点(超过4的半数2)在线。 乐观锁：取数据的时候都认为别人不会修改，不锁。更新数据的时候，先检查更新前的版本和自己之前读取的版本是否一致，一致则更新，否则重新读取。本质是CAS(Compare and Swap)。悲观锁：很悲观，取数据的时候认为别人会修改，所以取数据要上锁。api server以restful api提供对集群状态的CRUD操作，数据保存在etcd中。api server会对客户端身份做认证、授权，校验数据对象，对更新提供乐观锁处理的功能。可以配置一个或者多个插件，来对操作进行认证、授权、准入控制。资源变更的监控客户端到api server来监听在etcd上配置的变更，有变更时api server会通知客户端。kubectl get po --watch就可以监听pod的变化。Controller manager工作流程： 调度器来决定pod运行在哪个节点，调度器连接api server并监听，等待创建新pod。 决定在某个节点上创建新pod，并更新api server上的pod定义。 kubelet通过api server知道pod被调度到自己的节点，于是创建容器。Controller manager里有多种controller，包括ReplicaSet、DaemonSet controller，node controller, service controller, persistent volume controller…这些控制器之间不会直接通信，都和api server通信。可以看出前面的操作都在controller panel中进行，可通过 kubectl get events --watch 查看事件。Kubelet工作流程： 连接api server创建一个node资源来注册本节点。 watch api server是否给本节点分配pod，若分配则启动pod容器。 监控本地运行的容器，向api server报告它们的状态、事件和资源消耗。 容器出错时，重启容器 pod从api server中删除时，kubelet终止容器，并通知api server该pod已被终止。kubelet可以从api server获得需要运行的pod，也可以通过指定本地目录下的pod清单来运行pod。pod内会多创建一个container来保证Linux命名空间不变当pod运行时，首先会创建一个基础的container（COMMAND执行了pause），该容器通常和pod的生存周期相同，该容器的目的是确保这个pod运行后有一致的Linux namespace。因此，pod内有多个容器，或者容器在pod中被重启，也能保证他们有一致的命名空间。kube-proxy每个工作节点上除了有kubelet，还要运行kube-proxy。kube-proxy确保对service或者pod的访问都可以到达。kube-proxy有两种代理模式： userspace proxy。客户端-&amp;gt;iptables(kube-proxy配置iptables)-&amp;gt;kube-proxy-&amp;gt;pod。 iptables proxy。客户端-&amp;gt;iptables(kube-proxy配置iptables)-&amp;gt;pod。这种模式，数据包不经过kube-proxy。userspace proxy，对性能影响大，它以轮询模式选择pod做load balance。iptables proxy，随机选择pod。 kube-proxy使用iptables来转发数据包 监控 service 的创建当创建service时，虚拟IP地址就会分配给service。之后API服务器会通知所有在节点上的kube-proxy客户端有个新服务已经被创建了。然后，每个kube-proxy都会让该服务在自己的节点上可寻址。原理是通过创建iptables规则，确保每个目标为service的IP/端口被修改为支持服务的pod上。 监控 Endpoint 对象监控 Endpoint 来保证kube-proxy知道如何转发数据包到pod上。control panel 中组件挂掉时的选举选举时，这些组件不需要互相通信。领导者的选举方式是大家都尝试在api server创建一个endpoint对象，其中包含leader/holderIdentity字段指向自己，成功写入的就成为领导者。api server的乐观锁保证了并发只有一个会成功。多control panel或多组件时，领导者负责更新资源。当领导者宕机，其它组件发现资源超时也没有更新，就尝试将自己写到api server中成为领导者。k8s的安全防护认证和授权Authenticationpod使用ServiceAccount来表明自己的身份，一个ServiceAccount可以被多个pod使用。SA也是和Pod、ConfigMap等一样都是资源，每个namespace会有一个默认的ServiceAccount。# 创建SAkind: ServiceAccountmetadata: name: mysa# SA分配给podkind: Podspec: serviceAccountName: mysaAuthorizationk8s里包括一些授权插件，包括RBAC，ABAC(基于属性的访问控制)，WebHook插件等。 RBAC中的Role和ClusterRole定义了可以在哪些资源上执行什么操作。 RoleBinding 和 ClusterRoleBinding 将Role和ClusterRole绑定在用户、组和ServiceAccount。命名空间nspod中的容器有自己的network ns, PID ns, IPC ns。pod使用宿主节点的network nspod中的容器可直接看到和使用宿主节点的网络信息。kind: Podspec: hostNetwork: truepod只绑定宿主节点的端口，而不用宿主节点的整个network ns通过配置pod的spec.containers.ports字段中，某容器的hostPort属性来实现。这时，到达宿主节点该端口的连接会直接转发到pod的对应端口上。# 可以通过pod IP的8080访问，也可以通过节点的9090访问。kind: Podspec: containers: ports: - containerPort: 8080 hostPort: 9090 注意和 Service 的 NodePort 的区别，NodePort会将连接转发到随机选取的pod上，不管pod是不是在本节点。由于指定的端口会占用宿主机的端口，所以一个节点只能运行一个绑定某端口的pod，需要多个pod时，调度器会自动在其它节点上启动该pod，如果无多余的节点，则pod会pending。pod使用宿主节点的PID和IPC ns pod spec.hostPID为true时，pod内的容器可以看到宿主的全部进程信息。 pod spec.hostIPC为true时，pod内的容器可以使用IPC和宿主的进程通信。kind: Podspec: hostPID: true hostIPC: truesecurity-context对pod容器进行更细粒度的控制容器运行使用的用户可以在Dockerfile中通过USER来指定。如果没有指定，也没有配置security-context时，pod中的容器默认以root用户、root组运行。指定用户运行容器# runAsUser指定一个用户ID，不是用户名kind: Podspec: containers: securityContext: runAsUser: 405指定非root用户运行容器避免镜像被攻击后，运行恶意镜像，所以可以指定非root用户运行容器。kind: Podspec: containers: securityContext: runAsNonRoot: true特权模式运行podpod此时可以在宿主机上做任何事情，比如kube-proxy就可以修改宿主机的iptables。特权模式的pod可以看到宿主机的/dev，所以也就可以访问到宿主机的所有设备。kind: Podspec: containers: securityContext: privileged: true为容器指定可支持的内核功能privileged=true 给pod的权限太大，可以指定个别的内核功能给pod。# 该pod可以修改系统时间。需要把内核功能的 CAP_SYS_TIME 去掉 CAP_ 前缀。# add 添加功能，drop 去掉默认有的功能，比如chown文件的权限。kind: Podspec: securityContext: capabilities: add: - SYS_TIME drop: - CHOWN阻止容器对自己的根文件系统进行写入操作kind: Podspec: containers: securityContext: readOnlyRootFilesystem: truePodSecurityPolicy 进行集群级别的安全设置PodSecurityPolicy 资源无命名空间，定义了能否在pod中使用某种安全相关的特性。PodSecurityPolicy插件需要安装才可使用。# 容器不允许使用宿主IPC、PID、Network namespace# 容器只能使用10000-11000之间端口# 容器不能在特权模式下允许# 容器可以以任意用户、组运行# 容器可以使用任何SELinux选项kind: PodSecurityPolicyspec: hostIPC: false hostPID: false hostNetwork: false hostPorts: - min: 10000 max: 11000 privileged: false readOnlyRootFilesystem: true runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny supplementalGroups: rule: RunAsAny seLinux: rule: RunAsAnypod的网络隔离NetworkPolicy可以用标签选择器来匹配pod，或者用CIDR指定IP段，然后指定哪些ingress或者egress允许。 集群中的CNI或者网络方案需要支持NetworkPolicy。在一个ns中启用网络隔离默认情况下，pod可以被任意来源访问。可以启用对ns中所有pod的网络隔离，阻止所有访问。# podSelector 的value为空，则匹配ns中的所有podkind: NetworkPolicyspec: podSelector:对同一个ns中pod的访问# 标签app=mydb的pod只允许标签app=myapp的pod，且只能访问3306端口。myapp可直接或通过Service访问到mydb。kind: NetworkPolicyspec: podSelector: matchLabels: app: mydb ingress: - from: - podSelector: matchLabels: app: myapp ports - port: 3306不同ns中pod的访问将上面的 podSelector 变成了 namespaceSelector 来选择指定的ns。kind: NetworkPolicyspec: podSelector: matchLabels: app: mydb ingress: - from: - namespaceSelector: matchLabels: tenant: xxx ports - port: 3306限制pod的outbound流量# pod aaa，只能访问pod bbbkind: NetworkPolicyspec: podSelector: matchLabels: app: aaa egress: - to: - podSelector: matchLabels: app: bbb使用CIDR限制# 指明了inbound rule，来自192.168.0.0/24ingress:- from: - ipBlock: cidr: 192.168.0.0/24对资源的管理创建pod时，可以指定其中容器对CPU和内存的最小要求（requests），以及最大限制（limits）。kubelet会向API server报告节点相关数据。# 查看节点资源总量和已分配情况kubectl describe nodes为pod中的容器做资源分配requests设置容器的最小的需求requests的作用： 保证了该容器需要使用的最小资源。避免容器部署在资源不够的节点上。如果节点上可分配（注意不是可用）的资源小于requests的要求，则pod不会分配到该节点。比如已经有容器requests了80%的总CPU，哪怕实际只用了60%，也不会在该节点上部署requests 25%的pod。 当容器都全力使用CPU时，CPU用量会按照各个容器的requests的比例来分配。# CPU指定为200毫核，即一个CPU core的1/5.# memory申请100MB。kind: podspec: containers: - image: test resources: requests: cpu: 200m memory: 100Mi 调度器在调度时，可根据参数设置优先级：LeastRequestedPriority，优先将pod调度到requests少的节点。MostRequestedPriority，优先将pod调度到requests多的节点。可充分利用节点，释放不使用的节点以节省资金。limits设置容器的最大使用量 CPU是可压缩的资源，某个容器占用CPU高了，我们可以限制它的CPU用量。内存是不可压缩资源，如果进程申请了内存，除非主动释放，否则操作系统也不能让它主动释放内存（除非kill）。所以对内存进行限制，避免了单个pod故障导致整个节点不可用。limits的特点： 与requests不同，limits不受节点可分配资源量的限制，也就是可以超卖。 对CPU的限制，仅仅会让该容器分配不到比限额更多的CPU资源。 当进程尝试申请比限额更多的内存时，会被OOM kill。k8s会尝试重启，如果继续失败，会增加下次重启的时间间隔，从20s一直几何倍数增加到300s。 当然如果节点资源总量超过100%，一些容器会被kill。 注意：在pod内的容器，看到的CPU和内存数量是节点的，并不是limits后的。所以应用不能根据查询到的CPU和内存无限的申请用量。# CPU最大不能超过1 core# 由于没有指定requests，所以requests将会设置为和limits相同的值。kind: podspec: containers: - image: test resources: limits: cpu: 1 memory: 100Mipod的QoS由于limits设置的可以超卖，所以k8s有可能依据QoS选择pod kill掉。根据容器requests和limits的设置情况，将pod分为3个QoS等级： BestEffort（优先级最低）：没有配置requests和limits的pod，当资源不够时，最先被kill。 Guaranteed（优先级最高）：pod中的每个容器都设置了requests和limits，并且他们的值相同。 Burstable：除了前两种的其它pod。 当容器的QoS等级相同时，根据OOM分值kill掉分值高的pod。比如A requests 100MB，B requests 200MB，A实际用量150MB，B实际用量280MB，A先被kill，因为A超过它requests的比例更高。为命名空间的pod设置使用量LimitRangeLimitRange设置ns中requests和limits的默认值，以及最大最小值。避免了需要为每个pod设置资源限制。只有在LimitRange范围内的pod，才可以在该ns中被创建。但LimitRange只影响apply后创建的pod，对已创建的pod不影响。# 可以设置Pod、container和PVC。# maxLimitRequestRatio 指定request和limit的最大比例。kind: LimitRangespec: limits: - type: Pod min: cpu: 50m memory: 100Mi max: cpu: 1 memory: 1Gi - type: Containers defaultRequest: cpu: 100m memory: 10Mi default: cpu: 500m memory: 200Mi min: cpu: 50m memory: 100Mi max: cpu: 1 memory: 1Gi maxLimitRequestRatio: cpu: 4 memory: 5 - type: PersistentVolumeClaim min: storage: 1Gi max: storage: 10GiResourceQuotaResourceQuota可限制ns中所有pod允许使用的CPU、内存、PVC总量，以及可创建的对象数量。使用kubectl describe quota查看配额使用情况。# scopes设置quota的生效范围，比如BestEffort Qos的，以及没有有效期的pod上。kind: ResourceQuotaspec: scopes: - BestEffort - NotTerminating hard: requests.cpu: 500m requests.memory: 100Mi limits.cpu: 2 limits.memory: 500Mi requests.storage: 1Ti pods: 10 secrets: 20 persistentvolumeclaims: 5 services: 8 services.loadbalancers: 1监控资源使用量为了合理的设置requests和limits，需要监控pod的资源使用量。收集、获取资源使用情况kubelet中包含了一个 cAdvisor 的agent，会收集本节点和容器的资源消耗情况。可以在集群中运行一个 Heapster 组件来统计整个集群的监控信息。在Heapster运行了一会儿后，就可以通过下面的一些命令来获取信息：# 节点CPU和内存实际使用量，注意kubectl describe node看的是节点CPU和内存的requests和limitskubectl top node# pod CPU和内存的实际使用量kubectl top pod --all-namespaces# minikube需要安装插件minikube addons enable heapster持续监控历史资源使用的统计信息cAdvisor 和 Heapster 都只保存短时间内的数据，如果需要长时间的数据，可以用 InfluxDB 来储存数据，Grafana 对数据进行可视化和分析。InfluxDB 和 Grafana 可以用docker方式运行。pod和节点的自动伸缩pod的横向自动伸缩 (scale out)pod的横向auto scale由Horizontal controller执行，通过创建 HorizontalPodAutoscaler(HPA)来配置和启用。可基于CPU、内存、其它metrics来实现自动伸缩。HPA流程利用 cAdvisor 获取pod的信息，Heapster汇集，HPA从Heapster获取信息，计算后更新Deployment。基于CPU使用率自动伸缩要使用HPA，必须给pod设置CPU requests，这样HPA才能衡量CPU使用率。可以使用yaml定义资源，或者命令行直接修改。HPA衡量的CPU使用率 = 容器的CPU实际使用率 / 设置的CPU requests# autoscaler会调整副本的数量，使cpu使用率接近30%kubectl autoscale deployment xxx --cpu-percent=50 --min=1 --max=5# 查看HPAkubectl get hpakubectl describe hpa 伸缩操作的速率限制：单次扩容操作最多让当前副本数翻倍。两次扩容间也有时间限制，比如3分钟内没有伸缩操作才会扩容，5分钟内没有伸缩操作才会缩容。其它metrics可基于每秒查询次数（QPS）、平均响应时间，等等进行自动缩放。kind: HorizontalPodAutoscalerspec: maxReplicas: 5 minReplicas: 1 metrics: - type: Pods resource: name: qps targetAverageValue: 100缩容到0副本？目前HPA不支持缩容到0副本。也就是当没有请求的时候，缩容到0副本，有请求来时先被阻塞，直到pod启动，再转发请求到新pod上。未来k8s也许会支持。pod纵向缩放和节点的横向伸缩 pod纵向缩放，需查文档看k8s有没有实现，目前只能用替代方案。 节点的横向伸缩，需要云服务商的支持，目前GKE、GCE、Azure、AWS都支持节点的横向伸缩。高级调度Taints 和 Tolerations节点可以表明自己的污染度(Taints)，pod可以表明自己的容忍度(Tolerations)，符合条件的pod才会被部署到节点上。节点选择器虽然也有这样的功能，但是Taints可以在不修改已有pod信息的前提下，通过修改节点信息，来拒绝pod在某些节点上的部署。比如集群的主节点上设置了Taints，保证只有control panel pod可以部署在主节点上。pod的污染容忍度可以指定对污染的效果： NoSchedule：0容忍，pod不会调度到有这些污染的节点上。 PreferNoSchedule：是NoSchedule的宽松版本，尽量不调度到有这些污染的节点上，但如果没有别的节点，也会被调度上去。 NoExecute：前两者只在调度期间起作用，NoExecute会把节点上正在运行的pod去除。亲缘性affinity节点亲缘性虽然nodeSelector可以指定pod被调度到哪种节点上去，但是affinity更强大，可以指定pod对节点的硬性限制，或者偏好。kind: podspec: affinity: nodeAffinity: ...pod亲缘性pod亲缘性可以把前端pod和后端pod部署的尽量靠近。可以使用 matchLabels 和 matchExpressions 来匹配。requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution。# 部署的pod必须被调度到匹配的pod选择器的节点上。kind: Deploymentspec: replicas: 5 template: ... spec: affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: ... labelSelector: matchLabels: app: backendpod非亲缘性希望pod彼此远离的时候，就把 podAffinity 字段换成 podAntiAffinity。目的之一是避免在同一个节点上影响性能，之二是部署在不同节点满足高可用性的需求。Best practice上图是一个典型的k8s应用环境： 提供服务的pod通过 service 来暴露自己。集群外访问时，可以将service配置为 LoadBalancer 或者 NodePort 类型的，也可以通过 Ingress 资源来开放服务。 pod 模板通常会引用两种类型的Secret，一种用于从镜像仓库拉取镜像，另一种是pod中的进程使用的。Secret应由运维团队来配置，分配给 ServiceAccount，然后 SA 会被分配给pod。 一个应用也会包含一个或者多个 ConfigMap 对象，可以用 key value 来初始化环境变量，也可以在pod中以 configMap卷 来挂载。需要数据持久化的pod还需要 PVC卷。 一个应用可能会有 Jobs 和 CronJobs。DaemonSet通常由系统管理员创建，部署在各个节点上。 水平pod缩放（HorizontalPodAutoscaler）可由开发者或者运维团队配置。集群管理员可设置LimitRange和ResourceQuota，以控制每个pod和所有pod的资源使用情况。 资源通常会有多个标签，还应该有描述资源的注解，列出负责该资源的人员和团队的联系信息。pod的生命周期应用开发者的注意事项由于pod可能被k8s杀死或者重新调度，所以应用开发者需要注意： 预料到本地IP和主机名会发生变化。如果要依赖主机名，必须使用StatefulSet。 预料到写入容器磁盘的数据会消失。即使pod没有重新建，但容器重建，也会创建新的writable layer。需持久化的数据需要保存在PV中。应用需注意PV中的数据损坏，导致容器不停崩溃。控制pod的启动顺序虽然在一个yaml文件中定义所有的pod，也是按顺序发给api server的，但无法保证哪个先启动。控制启动顺序的方法： init容器：一个pod内可以包括多个init容器来完成初始化的工作，只有init容器运行结束后才会启动主容器。 使用 Readiness 探针检查依赖的pod。# 使用initContainers来定义init容器。spec: initContainers: - name: init image: xxx command: - sh - -c - &#39;sleep 5;&#39;容器启动后(Post-start)和停止前(Pre-stop)的钩子init容器是对应整个pod。此外可以在容器启动后和停止前添加钩子。 这里的钩子并不会在容器完全启动后才运行，而是和主进程并行执行。在钩子执行完毕之前，容器一直停留在Waiting状态，pod的状态会是Pending，而不是Running。如果钩子运行失败或者返回了非 0 的状态码，主容器会被kill。# 使用postStart和preStop来定义kind: podspec: containers: - image: xxx lifecycle: postStart: exec: command: - sh - -c - &quot;sleep 5;&quot;pod的终止当kubelet意识到需要终止pod时，它开始终止pod中的每个容器。kubelet会给每个容器一定的时间来终止（Termination Grace Period），可在pod spec中的 spec.terminationGracePeriod 的Periods字段来为每个容器设置不同值，默认为30s。其中kubelet终止pod的流程： 如果有 PreStop，执行它，并等待执行完毕。 向容器主进程发送 SIGTERM 信号。 等待容器关闭或者等待终止期限超时。 如果容器没有主动关闭，使用 SIGKILL 强制终止进程。 注意容器收到 SIGTERM 时，并不意味着pod一定会被终止，有可能只是重启其中的容器。kube-proxy很有可能收到的通知比kubelet收到的通知晚，导致容器已经停止服务了，之后kube-proxy才修改iptables，这样在中间的请求会收到access deny之类的错误。为了避免这样的问题，可以在容器的preStop里sleep几秒，这样iptables会先修改完，容器才会关闭。让应用在k8s里方便运行和管理 衡量镜像最小化和方便使用的关系。只有可执行持续的镜像让第一次部署就很快，但是没有常用命令方便调试。 镜像尽量指定版本。用latest可能让版本不受控制，不同的pod运行了不同的版本。 给所有的资源都打上标签。标签可以包含：资源所属的应用名称，应用层级（前后端等），运行环境（开发、测试、预发布、生产），版本号，发布类型（稳定、canary、蓝绿等），租户等。 给资源加上注解。包括资源描述和负责人。开发和测试的best practice开发过程中在k8s之外运行应用开发阶段，并不需要每次做完小的修改后都build docker，然后推送到registry，再deploy。 可以通过环境变量来连接后台的服务。 如果使用 ServiceAccount 来验证，可以把 SA 的secret文件用kubectl cp复制到本地。或者在本地运行kubelet proxy。 开发过程中需要在容器内部运行时，可以把本地目录通过docker的volume挂载到容器中，这样应用有新的build时，重启容器就可以，不需要重新构建整个镜像。使用minikube进行开发可以将自己的shell指向minikube的docker-daemon。这样构建完镜像，不用再去推送镜像，它已存储在Minikube VM中，可直接使用该镜像。eval $(minikube docker-env)CI/CD网上有很多资源，可以考虑 Fabric8 的项目。k8s应用扩展自定义资源 CustomResourceDefinition (CRD)如果添加一个服务需要定义Deployment、Service、ConfigMap等，步骤很繁琐，对重复操作，可以使用 CRD。# 定义CRDkind: CustomResourceDefinitionspec: scope: Namespacednames: kind: website ...定义了CRD后，则可使用上面定义的website作为kind类型创建资源。RedHat的Openshift(内容待补充)Helm(内容待补充)Centralized Logging (EFK)EFK(Elasticsearch, Fluentd, Kibana) Elasticsearch: a period, distributed, and a scalable computer program that permits for full-text and structured search, further as analytics. Kibana: a robust knowledge visualization frontend, and dashboard for Elasticsearch. Fluentd: gather, transform, and ship logs knowledge to the Elasticsearch backend.K8s tipsk8s修改资源的方式 kubectl edit: 使用默认编辑器打开资源配置，资源对象会被更新。kubectl edit deployment xxx kubectl patch: 修改少量资源属性。kubectl patch deployment xxx -p &#39;{&quot;spec&quot; : ...}&#39; kubectl apply: 使用一个完整的yaml或json文件，来更新或创建对象，需要包含资源的完整定义。kubectl apply -f xxx.yaml kubectl replace: 将现有对象替换为yaml或json定义的新对象，只能用来更新，不能创建。kubectl replace -f xxx.yaml kubectl set image: 修改pod、ReplicationController、Deployment、DaemonSet、Job、ReplicaSet中的镜像。kubectl set image deployment xxx container_name=aaaCommands命令行参数支持 tab 补全yum install -y bash-completionsource &amp;lt;(kubectl completion bash)source /etc/profile.d/bash_completion.sh查看相关配置的帮助kubectl explain replicaset.spec.replicas在运行的容器中远程执行命令。双横杠 – 代表 kubectl 命令的结束，后面的内容会在 pod 内部执行。kubectl exec pod_name -- ps -aux在pod中运行command，和Docker很像kubectl exec -it pod_name [-c container_name] -- bashminikube 查询addons，启用组件minikube addons listminikube addons enable ingress列出所有命名空间正在运行的 podkubectl get po --all-namespaces修改资源kubectl apply ...kubectl edit ...启动一个centos，并在后台等待kubectl run centos --image=centos -- sleep infinitykubectl exec centos -- lskubectl delete po centos在本机和容器间相互传送文件kubectl cp foo-pod:/container/aaa /local/aaakubectl cp foo-pod:/container/aaa /local/aaa -c container_name" }, { "title": "ksoftirqd occupied 100% CPU", "url": "/posts/ksoftirqd-occupied-100percent-CPU/", "categories": "性能调优", "tags": "ksoftirqd, performance", "date": "2021-09-06 13:00:00 +1000", "snippet": "IssueIn a performance testing, the Linux system is not in high loads (40% of total CPUs is in use), but our service can’t receive more requests and the ssh connections frequently disconnected.InvestigationThe testing environment was built on a KVM, and it was assigned 24 CPU cores. In top output, ksoftirqd/17 occupied 98% CPU. ksoftirqd is used to handle the interrupt from hardware, so maybe this process is the bottleneck of the system.There are 24 ksoftirqd processes related to 24 CPU cores, but only ksoftirqd/17 is very busy. So let’s check what ksoftirqd/17 is working for. /proc/interrupts recorded the interrupt information. Our NIC is ens3. We can see int 11 is working for ens3, and a large number of int 11 occured in CPU17, which is related with ksoftirqd/17.Obviously the NIC interrupts are not distributed to multiple CPUs. All NIC interrupts are only sent to ksoftirqd/17, and cause CPU 17 is too busy to process more packets.The other CPUs are not fully utilized, so we need to distribute the NIC interrupts to other CPUs to improve the system performance.Solve itRun ethtool -l ens3 to query NIC channels:# ethtool -l ens3Channel parameters for ens3:Cannot get device channel parameters: Operation not supportedIt looks the driver of NIC didn’t support multiple channels. Otherwise, we can use ethtool -L ens3 combined 8 to change channels, which means 8 cpu could handle soft irq.Irqbalance service is running, but it seems irqbalance can’t handle this case. We need to distribute the NIC interrupts manually by using Receive Packet Steering(RPS) and Receive Flow Steering(RFS).RPS works in Linux kernel to distribute NIC interrupts to multiple CPUs. RFS is to increase datacache hitrate by steering kernel processing of packets to the CPU where the application thread consuming the packet is running. The full document about network scaling could found at https://www.kernel.org/doc/Documentation/networking/scaling.txt.Just list the steps I did: Disable irqbalance service.# systemctl stop irqbalance# systemctl disable irqbalance Set SMP IRQ affinity. We could assign which CPUs can process the specific interrupt. In this VM, int 11 works for NIC interrupts, and I want all CPU cores could process the NIC interrupts.# echo &quot;0-23&quot; &amp;gt; /proc/irq/11/smp_affinity_list Alternatively, you could set the smp_affinity by hex value. We can check the last command by cat /proc/irq/11/smp_affinity, and get “ffffff”. Configure RPS. “ffffff” is a bitmap of CPUs.# echo &quot;ffffff&quot; &amp;gt; /sys/class/net/ens3/queues/rx-0/rps_cpus Configure RFS. For a single queue device, we set rps_sock_flow_entries and rps_flow_cnt value with the same value for a good performance.# sysctl -w net.core.rps_sock_flow_entries=32768# echo 32768 &amp;gt; /sys/class/net/ens3/queues/rx-0/rps_flow_cntAll doneThe NIC interrupts are distributed to multiple CPU cores, so our backend service can handle more requests and the whole system is fully utilized." }, { "title": "第一篇blog，windows下用jekyll docker写blog", "url": "/posts/write-blog-in-windows/", "categories": "建站", "tags": "docker, jekyll", "date": "2021-06-27 12:00:00 +1000", "snippet": "准确的说并不是用jekyll写blog，而是用jekyll docker在本地验证自己的blog，然后好推送到github page上去。需求：我想有个blog一直想建个blog记录东西，可以当作笔记一样记录工作和学习中的一些足迹，还可以分享。之前也简单调研过一些建blog的信息，说说我的感受，主要是缺点哈：:joy:wordpress用wordpress、joomla之类的感觉太重，虚拟主机便宜且操作起来简单，但是还是需要自己维护，包括安全性、备份等。blog内容和图片等在MySQL上，以后要是迁移到别的系统，可能都有未知的坑。感觉又要依赖MySQL里的数据，又要依赖wordpress本身，缺一个，数据的重新获取就会有麻烦。想到了西红柿炒鸡蛋，如果西红柿和鸡蛋分别放在两个篮子里，那有一个篮子出了问题，西红柿炒鸡蛋就做不出来了，而两个篮子里任意一个出问题的概率比一个篮子要大。简书CSDN、简书、头条之类的blog，感觉界面上有些东西自己不可控，比如广告、弹窗、推送什么的，体验不好。有些人可能会觉得这上面的推荐也会给自己带来流量，这个仁者见仁智者见智了，比起那一点流量来说，我还是喜欢给自己一个更清爽的浏览界面。Github pages因为我的blog会记录些技术的东西，所以 Github pages 当时也有考虑，它有两种建blog的方式： 以jekyll为代表，提交markdown文档和脚本到github，由github来编译并发布。 以Hugo为代表，在本机编译好页面后，提交到github。我倾向于第一种方式，提交markdown文件上去由github来编译，否则用Hugo编译的话本地需要保存原始的markdown文件和编译后的页面文件。但是我当时在windows环境折腾jekyll，各种版本兼容性问题，我对ruby又不熟，最后放弃了，毕竟我是想用blog来记录，而不是想折腾blog :rofl:不是blog的笔记本 可能是我要求太多了：支持markdown，编辑方便，跨iphone、安卓、windows、mac无缝同步。 印象笔记：缺点是分享不方便，手机端不能编辑markdown的笔记。不过之前一直在用它，还充了很久会员，就当网页保存器吧。 Notion：好看，但是担心公司小，哪天数据没了，就算文本和图片都有备份，如何导入到别的系统也会有麻烦。最终方案前几天偶然看到jekyll有docker版了，这样就又可以用jekyll，又不用在windows环境折腾ruby和jekyll的库了。最终找到一个blog方案可以满足我的全部要求了：:smiley: 本地测试页面简单，一条docker语句就好。 markdown里面的图片链接用相对路径，这样只要有markdown文件和图片，很容易切换到别的环境建站。 可以离线写，github用来保存每次变更，相当于备份了。 编辑markdown方便，vscode+extensions，预览、插图、版本管理等等在windows和mac下可以有一样的体验。 在github pages上也可以很容易就实现：评论，统计，搜索，打赏，广告等等。一句话，实在找不到不用github pages的理由，还免费 :+1:大概的流程 安装Docker Desktop for Windows 安装Windows Subsystem for Linux (WSL)，指定WSL2为默认。 选择一套jekyll模板，我用的是jekyll-theme-chirpy，可以通过该链接来使用chirpy模板。注意第一次需要参考链接里面的步骤来部署到github pages。 参考下面的命令用docker启动jekyll。相关命令 在本地进入jekyll项目的目录后，下面的命令会启动jekyll serve，成功后就可以通过4000端口访问本地的开启的web服务：docker run --rm --label=jekyll --volume=&quot;%CD%:/srv/jekyll&quot; -p 4000:4000 jekyll/jekyll:4.2.0 jekyll serve类似的，可以用 jekyll build，或者给serve添加watch, trace等参数。 如果环境里面有错误，可以通过下面的命令进入jekyll docker查看。docker run --rm --label=jekyll --volume=&quot;%CD%:/srv/jekyll&quot; -it jekyll/jekyll:4.2.0 /bin/bash常见问题 提示 /srv/jekyll 目录没有权限：jekyll 3.9.0 | Error: Operation not permitted @ apply2files - /srv/jekyll/usr/local/lib/ruby/2.7.0/fileutils.rb:1346:in `chmod&#39;: Operation not permitted @ apply2files - /srv/jekyll (Errno::EPERM)需要进入到jekyll docker中将/srv/jekyll目录改为jekyll用户：docker run --rm --label=jekyll --volume=&quot;%CD%:/srv/jekyll&quot; -it jekyll/jekyll:4.2.0 /bin/bashchown -R jekyll:jekyll /srv/jekyll" } ]
